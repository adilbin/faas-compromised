{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Transformer with All Features Combined\n",
    "\n",
    "This notebook implements a Transformer Encoder for syscall-based malware detection using **all features combined**:\n",
    "- **Syscall**: Categorical feature (embedded)\n",
    "- **Return Value (Ret)**: Categorical feature (embedded)\n",
    "- **Parameters**: Text feature (sentence transformer embeddings)\n",
    "\n",
    "The three feature representations are concatenated at each timestep and projected to a hidden dimension before being fed to the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading sentence transformer model...\n",
      "Parameter embedding dimension: 384\n",
      "Total combined embedding dimension: 448\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../configs')\n",
    "from config_loader import get_split_with_labels\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Config\n",
    "SPLIT = '70'\n",
    "WINDOW_SIZES = [2000]  # Different sliding window lengths to test\n",
    "# WINDOW_SIZES = [250, 500, 1000, 2000]  # Different sliding window lengths to test\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 6\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Embedding dimensions\n",
    "SYSCALL_EMBED_DIM = 32\n",
    "RETVAL_EMBED_DIM = 32\n",
    "\n",
    "# Load sentence transformer model for parameter embeddings\n",
    "print(\"Loading sentence transformer model...\")\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)\n",
    "PARAM_EMBED_DIM = sentence_model.get_sentence_embedding_dimension()\n",
    "print(f\"Parameter embedding dimension: {PARAM_EMBED_DIM}\")\n",
    "print(f\"Total combined embedding dimension: {SYSCALL_EMBED_DIM + RETVAL_EMBED_DIM + PARAM_EMBED_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 21, Test files: 9\n",
      "\n",
      "Training set:\n",
      "  Total runs: 1986\n",
      "  Benign runs: 1484\n",
      "  Malicious runs: 502\n",
      "\n",
      "Test set:\n",
      "  Total runs: 810\n",
      "  Benign runs: 489\n",
      "  Malicious runs: 321\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_files, test_files = get_split_with_labels(SPLIT)\n",
    "print(f\"Train files: {len(train_files)}, Test files: {len(test_files)}\")\n",
    "\n",
    "def load_runs_all_features(file_path):\n",
    "    \"\"\"Load all features (syscall, ret, parameters) grouped by run.\n",
    "    \n",
    "    Returns list of runs, where each run is a dict with:\n",
    "        - 'syscalls': list of syscall names\n",
    "        - 'retvals': list of return values\n",
    "        - 'params': list of parameter strings\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    runs = []\n",
    "    for run_id, group in df.groupby('run'):\n",
    "        run_data = {\n",
    "            'syscalls': group['syscall'].tolist(),\n",
    "            'retvals': group['Ret'].tolist(),\n",
    "            'params': group['parameters'].tolist()\n",
    "        }\n",
    "        runs.append(run_data)\n",
    "    return runs\n",
    "\n",
    "# Count runs per label for training and test sets\n",
    "def count_runs_per_label(file_label_pairs):\n",
    "    \"\"\"Count total runs per label.\"\"\"\n",
    "    counts = {'benign': 0, 'malicious': 0}\n",
    "    for path, label in file_label_pairs:\n",
    "        runs = load_runs_all_features(path)\n",
    "        counts[label] += len(runs)\n",
    "    return counts\n",
    "\n",
    "train_counts = count_runs_per_label(train_files)\n",
    "test_counts = count_runs_per_label(test_files)\n",
    "\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Total runs: {sum(train_counts.values())}\")\n",
    "print(f\"  Benign runs: {train_counts['benign']}\")\n",
    "print(f\"  Malicious runs: {train_counts['malicious']}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Total runs: {sum(test_counts.values())}\")\n",
    "print(f\"  Benign runs: {test_counts['benign']}\")\n",
    "print(f\"  Malicious runs: {test_counts['malicious']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "build_encoders",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoders for syscalls and return values...\n",
      "Syscall vocabulary size: 81 (including PAD)\n",
      "Return value vocabulary size: 42586 (including PAD)\n",
      "\n",
      "Unique parameter strings: 266019\n"
     ]
    }
   ],
   "source": [
    "# Build encoders for syscalls and return values\n",
    "print(\"Building encoders for syscalls and return values...\")\n",
    "\n",
    "all_syscalls = []\n",
    "all_retvals = []\n",
    "all_params = set()\n",
    "\n",
    "for path, _ in train_files + test_files:\n",
    "    for run_data in load_runs_all_features(path):\n",
    "        all_syscalls.extend(run_data['syscalls'])\n",
    "        all_retvals.extend(run_data['retvals'])\n",
    "        for param in run_data['params']:\n",
    "            if pd.isna(param):\n",
    "                all_params.add('<EMPTY>')\n",
    "            else:\n",
    "                all_params.add(str(param))\n",
    "\n",
    "# Build syscall encoder\n",
    "syscall_encoder = LabelEncoder()\n",
    "syscall_encoder.fit(all_syscalls)\n",
    "syscall_vocab_size = len(syscall_encoder.classes_) + 1  # +1 for PAD token\n",
    "print(f\"Syscall vocabulary size: {syscall_vocab_size} (including PAD)\")\n",
    "\n",
    "# Build return value encoder\n",
    "retval_encoder = LabelEncoder()\n",
    "retval_encoder.fit(all_retvals)\n",
    "retval_vocab_size = len(retval_encoder.classes_) + 1  # +1 for PAD token\n",
    "print(f\"Return value vocabulary size: {retval_vocab_size} (including PAD)\")\n",
    "\n",
    "# PAD index for embeddings\n",
    "PAD_IDX = 0\n",
    "\n",
    "print(f\"\\nUnique parameter strings: {len(all_params)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "param_embeddings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence embeddings for parameters (this may take a few minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37725214bb584043a6158523bdb020b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter embeddings computed. Shape per embedding: 384\n"
     ]
    }
   ],
   "source": [
    "# Pre-compute sentence embeddings for all unique parameter strings\n",
    "print(\"Computing sentence embeddings for parameters (this may take a few minutes)...\")\n",
    "\n",
    "all_params_list = list(all_params)\n",
    "param_embeddings = sentence_model.encode(\n",
    "    all_params_list,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=256,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "# Create a mapping from parameter string to embedding\n",
    "param_to_embedding = {param: emb for param, emb in zip(all_params_list, param_embeddings)}\n",
    "print(f\"Parameter embeddings computed. Shape per embedding: {PARAM_EMBED_DIM}\")\n",
    "\n",
    "# Create zero embedding for padding\n",
    "PAD_PARAM_EMBEDDING = np.zeros(PARAM_EMBED_DIM, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dataset_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllFeaturesDataset(Dataset):\n",
    "    \"\"\"Dataset that combines syscall, return value, and parameter features.\n",
    "    \n",
    "    For each timestep, we have:\n",
    "    - Syscall index (to be embedded by the model)\n",
    "    - Return value index (to be embedded by the model)\n",
    "    - Parameter embedding (pre-computed sentence embedding)\n",
    "    \"\"\"\n",
    "    def __init__(self, file_label_pairs, syscall_encoder, retval_encoder, \n",
    "                 param_to_embedding, window_size, param_embed_dim, pad_param_embedding):\n",
    "        self.syscall_seqs = []\n",
    "        self.retval_seqs = []\n",
    "        self.param_embeddings = []\n",
    "        self.labels = []\n",
    "        label_map = {'benign': 0, 'malicious': 1}\n",
    "        \n",
    "        for path, label in file_label_pairs:\n",
    "            runs = load_runs_all_features(path)\n",
    "            for run_data in runs:\n",
    "                syscalls = run_data['syscalls']\n",
    "                retvals = run_data['retvals']\n",
    "                params = run_data['params']\n",
    "                \n",
    "                seq_len = min(len(syscalls), window_size)\n",
    "                \n",
    "                # Encode syscalls (+1 to reserve 0 for PAD)\n",
    "                encoded_syscalls = syscall_encoder.transform(syscalls[:seq_len]) + 1\n",
    "                if len(encoded_syscalls) < window_size:\n",
    "                    encoded_syscalls = np.pad(encoded_syscalls, \n",
    "                                              (0, window_size - len(encoded_syscalls)), \n",
    "                                              constant_values=PAD_IDX)\n",
    "                \n",
    "                # Encode return values (+1 to reserve 0 for PAD)\n",
    "                encoded_retvals = retval_encoder.transform(retvals[:seq_len]) + 1\n",
    "                if len(encoded_retvals) < window_size:\n",
    "                    encoded_retvals = np.pad(encoded_retvals, \n",
    "                                             (0, window_size - len(encoded_retvals)), \n",
    "                                             constant_values=PAD_IDX)\n",
    "                \n",
    "                # Get parameter embeddings\n",
    "                param_embs = []\n",
    "                for i in range(window_size):\n",
    "                    if i < len(params):\n",
    "                        param = params[i]\n",
    "                        if pd.isna(param):\n",
    "                            key = '<EMPTY>'\n",
    "                        else:\n",
    "                            key = str(param)\n",
    "                        param_embs.append(param_to_embedding[key])\n",
    "                    else:\n",
    "                        param_embs.append(pad_param_embedding)\n",
    "                \n",
    "                self.syscall_seqs.append(encoded_syscalls)\n",
    "                self.retval_seqs.append(encoded_retvals)\n",
    "                self.param_embeddings.append(np.array(param_embs, dtype=np.float32))\n",
    "                self.labels.append(label_map[label])\n",
    "        \n",
    "        self.syscall_seqs = np.array(self.syscall_seqs)\n",
    "        self.retval_seqs = np.array(self.retval_seqs)\n",
    "        self.param_embeddings = np.array(self.param_embeddings)\n",
    "        self.labels = np.array(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.syscall_seqs[idx], dtype=torch.long),\n",
    "            torch.tensor(self.retval_seqs[idx], dtype=torch.long),\n",
    "            torch.tensor(self.param_embeddings[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "model_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderAllFeatures(nn.Module):\n",
    "    \"\"\"Transformer Encoder that combines all features:\n",
    "    - Syscall embeddings (learned)\n",
    "    - Return value embeddings (learned)\n",
    "    - Parameter embeddings (pre-computed sentence embeddings)\n",
    "    \n",
    "    All three are concatenated at each timestep, projected to hidden dim,\n",
    "    and then fed to the Transformer encoder with positional encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, syscall_vocab_size, retval_vocab_size,\n",
    "                 syscall_embed_dim, retval_embed_dim, param_embed_dim,\n",
    "                 max_len, hidden_dim=128, num_heads=4, num_layers=2, \n",
    "                 ff_dim=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers for categorical features\n",
    "        self.syscall_embedding = nn.Embedding(syscall_vocab_size, syscall_embed_dim, padding_idx=PAD_IDX)\n",
    "        self.retval_embedding = nn.Embedding(retval_vocab_size, retval_embed_dim, padding_idx=PAD_IDX)\n",
    "        \n",
    "        # Total input dimension after concatenation\n",
    "        total_embed_dim = syscall_embed_dim + retval_embed_dim + param_embed_dim\n",
    "        \n",
    "        # Project concatenated embeddings to hidden dimension\n",
    "        self.input_projection = nn.Linear(total_embed_dim, hidden_dim)\n",
    "        \n",
    "        # Learnable positional encoding\n",
    "        self.pos_encoding = nn.Parameter(torch.zeros(1, max_len, hidden_dim))\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.pos_encoding, std=0.02)\n",
    "    \n",
    "    def forward(self, syscalls, retvals, param_embs):\n",
    "        # Embed categorical features\n",
    "        syscall_emb = self.syscall_embedding(syscalls)  # (batch, seq_len, syscall_embed_dim)\n",
    "        retval_emb = self.retval_embedding(retvals)      # (batch, seq_len, retval_embed_dim)\n",
    "        \n",
    "        # param_embs already has shape (batch, seq_len, param_embed_dim)\n",
    "        \n",
    "        # Concatenate all embeddings\n",
    "        x = torch.cat([syscall_emb, retval_emb, param_embs], dim=2)  # (batch, seq_len, total_embed_dim)\n",
    "        \n",
    "        # Create padding mask based on syscalls (PAD positions have index 0)\n",
    "        padding_mask = (syscalls == PAD_IDX)  # True for padded positions\n",
    "        \n",
    "        # Project to hidden dimension and add positional encoding\n",
    "        x = self.input_projection(x) + self.pos_encoding[:, :x.size(1), :]\n",
    "        \n",
    "        # Transformer encoder\n",
    "        x = self.transformer(x, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # Global average pooling (excluding padding)\n",
    "        mask_expanded = (~padding_mask).unsqueeze(-1).float()\n",
    "        x = (x * mask_expanded).sum(dim=1) / (mask_expanded.sum(dim=1) + 1e-9)\n",
    "        \n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "experiments",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPERIMENT: Window Size = 2000\n",
      "============================================================\n",
      "Train samples: 1986, Test samples: 810\n",
      "\n",
      "--- Input Shapes ---\n",
      "  Syscall input shape:   (batch_size, 2000)\n",
      "  Retval input shape:    (batch_size, 2000)\n",
      "  Param emb input shape: (batch_size, 2000, 384)\n",
      "  After embedding concat: (batch_size, 2000, 448)\n",
      "\n",
      "--- Model Architecture & Parameters ---\n",
      "  pos_encoding: [1, 2000, 128] = 256,000 params\n",
      "  syscall_embedding.weight: [81, 32] = 2,592 params\n",
      "  retval_embedding.weight: [42586, 32] = 1,362,752 params\n",
      "  input_projection.weight: [128, 448] = 57,344 params\n",
      "  input_projection.bias: [128] = 128 params\n",
      "  transformer.layers.0.self_attn.in_proj_weight: [384, 128] = 49,152 params\n",
      "  transformer.layers.0.self_attn.in_proj_bias: [384] = 384 params\n",
      "  transformer.layers.0.self_attn.out_proj.weight: [128, 128] = 16,384 params\n",
      "  transformer.layers.0.self_attn.out_proj.bias: [128] = 128 params\n",
      "  transformer.layers.0.linear1.weight: [256, 128] = 32,768 params\n",
      "  transformer.layers.0.linear1.bias: [256] = 256 params\n",
      "  transformer.layers.0.linear2.weight: [128, 256] = 32,768 params\n",
      "  transformer.layers.0.linear2.bias: [128] = 128 params\n",
      "  transformer.layers.0.norm1.weight: [128] = 128 params\n",
      "  transformer.layers.0.norm1.bias: [128] = 128 params\n",
      "  transformer.layers.0.norm2.weight: [128] = 128 params\n",
      "  transformer.layers.0.norm2.bias: [128] = 128 params\n",
      "  transformer.layers.1.self_attn.in_proj_weight: [384, 128] = 49,152 params\n",
      "  transformer.layers.1.self_attn.in_proj_bias: [384] = 384 params\n",
      "  transformer.layers.1.self_attn.out_proj.weight: [128, 128] = 16,384 params\n",
      "  transformer.layers.1.self_attn.out_proj.bias: [128] = 128 params\n",
      "  transformer.layers.1.linear1.weight: [256, 128] = 32,768 params\n",
      "  transformer.layers.1.linear1.bias: [256] = 256 params\n",
      "  transformer.layers.1.linear2.weight: [128, 256] = 32,768 params\n",
      "  transformer.layers.1.linear2.bias: [128] = 128 params\n",
      "  transformer.layers.1.norm1.weight: [128] = 128 params\n",
      "  transformer.layers.1.norm1.bias: [128] = 128 params\n",
      "  transformer.layers.1.norm2.weight: [128] = 128 params\n",
      "  transformer.layers.1.norm2.bias: [128] = 128 params\n",
      "  fc.0.weight: [64, 128] = 8,192 params\n",
      "  fc.0.bias: [64] = 64 params\n",
      "  fc.3.weight: [2, 64] = 128 params\n",
      "  fc.3.bias: [2] = 2 params\n",
      "  ──────────────────────────────────────────────────\n",
      "  Total parameters:     1,952,162\n",
      "  Trainable parameters: 1,952,162\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|██████████| 63/63 [27:39<00:00, 26.34s/it, loss=0.0078, acc=0.9149]\n",
      "Epoch 2/6: 100%|██████████| 63/63 [27:39<00:00, 26.34s/it, loss=0.0041, acc=0.9658]\n",
      "Epoch 3/6: 100%|██████████| 63/63 [27:39<00:00, 26.34s/it, loss=0.0034, acc=0.9612]\n",
      "Epoch 4/6: 100%|██████████| 63/63 [27:39<00:00, 26.34s/it, loss=0.0004, acc=0.9990]\n",
      "Epoch 5/6: 100%|██████████| 63/63 [27:39<00:00, 26.34s/it, loss=0.0003, acc=0.9985]\n",
      "Epoch 6/6: 100%|██████████| 63/63 [27:39<00:00, 26.34s/it, loss=0.0002, acc=0.9990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 9956.35s\n",
      "\n",
      "Evaluating...\n",
      "Test time: 25.86s\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       1.00      0.81      0.90       489\n",
      "   malicious       0.78      1.00      0.87       321\n",
      "\n",
      "    accuracy                           0.89       810\n",
      "   macro avg       0.89      0.91      0.89       810\n",
      "weighted avg       0.91      0.89      0.89       810\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred: benign  Pred: malicious\n",
      "True: benign              397               92\n",
      "True: malicious             0              321\n",
      "\n",
      "Detection Rate: 1.0000\n",
      "False Positive Rate: 0.1881\n",
      "F1-score (weighted): 0.8876\n"
     ]
    }
   ],
   "source": [
    "# Run experiments with different window sizes\n",
    "results = []\n",
    "\n",
    "for window_size in WINDOW_SIZES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXPERIMENT: Window Size = {window_size}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = AllFeaturesDataset(\n",
    "        train_files, syscall_encoder, retval_encoder,\n",
    "        param_to_embedding, window_size, PARAM_EMBED_DIM, PAD_PARAM_EMBEDDING\n",
    "    )\n",
    "    test_dataset = AllFeaturesDataset(\n",
    "        test_files, syscall_encoder, retval_encoder,\n",
    "        param_to_embedding, window_size, PARAM_EMBED_DIM, PAD_PARAM_EMBEDDING\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Print input shapes\n",
    "    print(f\"\\n--- Input Shapes ---\")\n",
    "    print(f\"  Syscall input shape:   (batch_size, {window_size})\")\n",
    "    print(f\"  Retval input shape:    (batch_size, {window_size})\")\n",
    "    print(f\"  Param emb input shape: (batch_size, {window_size}, {PARAM_EMBED_DIM})\")\n",
    "    print(f\"  After embedding concat: (batch_size, {window_size}, {SYSCALL_EMBED_DIM + RETVAL_EMBED_DIM + PARAM_EMBED_DIM})\")\n",
    "    \n",
    "    # Create model\n",
    "    model = TransformerEncoderAllFeatures(\n",
    "        syscall_vocab_size=syscall_vocab_size,\n",
    "        retval_vocab_size=retval_vocab_size,\n",
    "        syscall_embed_dim=SYSCALL_EMBED_DIM,\n",
    "        retval_embed_dim=RETVAL_EMBED_DIM,\n",
    "        param_embed_dim=PARAM_EMBED_DIM,\n",
    "        max_len=window_size,\n",
    "        hidden_dim=128,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "        ff_dim=256,\n",
    "        dropout=0.1\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Print model parameter sizes per layer\n",
    "    print(f\"\\n--- Model Architecture & Parameters ---\")\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        param_count = param.numel()\n",
    "        total_params += param_count\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param_count\n",
    "        print(f\"  {name}: {list(param.shape)} = {param_count:,} params\")\n",
    "    print(f\"  {'─'*50}\")\n",
    "    print(f\"  Total parameters:     {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Training\n",
    "    print(f\"\\nTraining...\")\n",
    "    train_start_time = time.time()\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=True)\n",
    "        for syscalls, retvals, param_embs, labels in pbar:\n",
    "            syscalls = syscalls.to(DEVICE)\n",
    "            retvals = retvals.to(DEVICE)\n",
    "            param_embs = param_embs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(syscalls, retvals, param_embs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{total_loss/total:.4f}', 'acc': f'{correct/total:.4f}'})\n",
    "        \n",
    "        train_acc = correct / total\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    train_time = time.time() - train_start_time\n",
    "    print(f\"Training time: {train_time:.2f}s\")\n",
    "    \n",
    "    # Testing\n",
    "    print(f\"\\nEvaluating...\")\n",
    "    test_start_time = time.time()\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for syscalls, retvals, param_embs, labels in test_loader:\n",
    "            syscalls = syscalls.to(DEVICE)\n",
    "            retvals = retvals.to(DEVICE)\n",
    "            param_embs = param_embs.to(DEVICE)\n",
    "            \n",
    "            outputs = model(syscalls, retvals, param_embs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    test_time = time.time() - test_start_time\n",
    "    print(f\"Test time: {test_time:.2f}s\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Window Size': window_size,\n",
    "        'Detection Rate': detection_rate,\n",
    "        'False Positive Rate': false_positive_rate,\n",
    "        'F1-score (weighted)': f1_weighted,\n",
    "        'Train Time (s)': train_time,\n",
    "        'Test Time (s)': test_time\n",
    "    })\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['benign', 'malicious']))\n",
    "    \n",
    "    labels_names = ['benign', 'malicious']\n",
    "    cm_df = pd.DataFrame(cm, index=[f'True: {l}' for l in labels_names], columns=[f'Pred: {l}' for l in labels_names])\n",
    "    print(f\"Confusion Matrix:\")\n",
    "    print(cm_df)\n",
    "    \n",
    "    print(f\"\\nDetection Rate: {detection_rate:.4f}\")\n",
    "    print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n",
    "    print(f\"F1-score (weighted): {f1_weighted:.4f}\")\n",
    "    \n",
    "    # Clear GPU memory between experiments\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb13476",
   "metadata": {},
   "source": [
    "### Earlier results for 250, 500, and 1000 window size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8822bf4",
   "metadata": {},
   "source": [
    "```plaintext\n",
    "\n",
    "============================================================\n",
    "EXPERIMENT: Window Size = 250\n",
    "============================================================\n",
    "Train samples: 1986, Test samples: 810\n",
    "\n",
    "--- Input Shapes ---\n",
    "  Syscall input shape:   (batch_size, 250)\n",
    "  Retval input shape:    (batch_size, 250)\n",
    "  Param emb input shape: (batch_size, 250, 384)\n",
    "  After embedding concat: (batch_size, 250, 448)\n",
    "\n",
    "--- Model Architecture & Parameters ---\n",
    "  pos_encoding: [1, 250, 128] = 32,000 params\n",
    "  syscall_embedding.weight: [81, 32] = 2,592 params\n",
    "  retval_embedding.weight: [42586, 32] = 1,362,752 params\n",
    "  input_projection.weight: [128, 448] = 57,344 params\n",
    "  input_projection.bias: [128] = 128 params\n",
    "  transformer.layers.0.self_attn.in_proj_weight: [384, 128] = 49,152 params\n",
    "  transformer.layers.0.self_attn.in_proj_bias: [384] = 384 params\n",
    "  transformer.layers.0.self_attn.out_proj.weight: [128, 128] = 16,384 params\n",
    "  transformer.layers.0.self_attn.out_proj.bias: [128] = 128 params\n",
    "  transformer.layers.0.linear1.weight: [256, 128] = 32,768 params\n",
    "  transformer.layers.0.linear1.bias: [256] = 256 params\n",
    "  transformer.layers.0.linear2.weight: [128, 256] = 32,768 params\n",
    "  transformer.layers.0.linear2.bias: [128] = 128 params\n",
    "  transformer.layers.0.norm1.weight: [128] = 128 params\n",
    "  transformer.layers.0.norm1.bias: [128] = 128 params\n",
    "  transformer.layers.0.norm2.weight: [128] = 128 params\n",
    "  transformer.layers.0.norm2.bias: [128] = 128 params\n",
    "  transformer.layers.1.self_attn.in_proj_weight: [384, 128] = 49,152 params\n",
    "  transformer.layers.1.self_attn.in_proj_bias: [384] = 384 params\n",
    "  transformer.layers.1.self_attn.out_proj.weight: [128, 128] = 16,384 params\n",
    "  transformer.layers.1.self_attn.out_proj.bias: [128] = 128 params\n",
    "  transformer.layers.1.linear1.weight: [256, 128] = 32,768 params\n",
    "  transformer.layers.1.linear1.bias: [256] = 256 params\n",
    "  transformer.layers.1.linear2.weight: [128, 256] = 32,768 params\n",
    "  transformer.layers.1.linear2.bias: [128] = 128 params\n",
    "  transformer.layers.1.norm1.weight: [128] = 128 params\n",
    "  transformer.layers.1.norm1.bias: [128] = 128 params\n",
    "  transformer.layers.1.norm2.weight: [128] = 128 params\n",
    "  transformer.layers.1.norm2.bias: [128] = 128 params\n",
    "  fc.0.weight: [64, 128] = 8,192 params\n",
    "  fc.0.bias: [64] = 64 params\n",
    "  fc.3.weight: [2, 64] = 128 params\n",
    "  fc.3.bias: [2] = 2 params\n",
    "  ──────────────────────────────────────────────────\n",
    "  Total parameters:     1,728,162\n",
    "  Trainable parameters: 1,728,162\n",
    "\n",
    "Training...\n",
    "Epoch 1/6: 100%|██████████| 63/63 [00:19<00:00,  3.26it/s, loss=0.0081, acc=0.9038]\n",
    "Epoch 2/6: 100%|██████████| 63/63 [00:20<00:00,  3.07it/s, loss=0.0010, acc=0.9950]\n",
    "Epoch 3/6: 100%|██████████| 63/63 [00:20<00:00,  3.04it/s, loss=0.0009, acc=0.9955]\n",
    "Epoch 4/6: 100%|██████████| 63/63 [00:20<00:00,  3.02it/s, loss=0.0009, acc=0.9955]\n",
    "Epoch 5/6: 100%|██████████| 63/63 [00:20<00:00,  3.02it/s, loss=0.0009, acc=0.9955]\n",
    "Epoch 6/6: 100%|██████████| 63/63 [00:20<00:00,  3.03it/s, loss=0.0020, acc=0.9955]\n",
    "Training time: 123.06s\n",
    "\n",
    "Evaluating...\n",
    "Test time: 2.12s\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      benign       0.99      0.82      0.90       489\n",
    "   malicious       0.79      0.99      0.88       321\n",
    "\n",
    "    accuracy                           0.89       810\n",
    "   macro avg       0.89      0.91      0.89       810\n",
    "weighted avg       0.91      0.89      0.89       810\n",
    "\n",
    "Confusion Matrix:\n",
    "                 Pred: benign  Pred: malicious\n",
    "True: benign              402               87\n",
    "True: malicious             3              318\n",
    "\n",
    "Detection Rate: 0.9907\n",
    "False Positive Rate: 0.1779\n",
    "F1-score (weighted): 0.8901\n",
    "\n",
    "============================================================\n",
    "EXPERIMENT: Window Size = 500\n",
    "============================================================\n",
    "Train samples: 1986, Test samples: 810\n",
    "\n",
    "--- Input Shapes ---\n",
    "  Syscall input shape:   (batch_size, 500)\n",
    "  Retval input shape:    (batch_size, 500)\n",
    "  Param emb input shape: (batch_size, 500, 384)\n",
    "  After embedding concat: (batch_size, 500, 448)\n",
    "\n",
    "--- Model Architecture & Parameters ---\n",
    "  pos_encoding: [1, 500, 128] = 64,000 params\n",
    "  syscall_embedding.weight: [81, 32] = 2,592 params\n",
    "  retval_embedding.weight: [42586, 32] = 1,362,752 params\n",
    "  input_projection.weight: [128, 448] = 57,344 params\n",
    "  input_projection.bias: [128] = 128 params\n",
    "  transformer.layers.0.self_attn.in_proj_weight: [384, 128] = 49,152 params\n",
    "  transformer.layers.0.self_attn.in_proj_bias: [384] = 384 params\n",
    "  transformer.layers.0.self_attn.out_proj.weight: [128, 128] = 16,384 params\n",
    "  transformer.layers.0.self_attn.out_proj.bias: [128] = 128 params\n",
    "  transformer.layers.0.linear1.weight: [256, 128] = 32,768 params\n",
    "  transformer.layers.0.linear1.bias: [256] = 256 params\n",
    "  transformer.layers.0.linear2.weight: [128, 256] = 32,768 params\n",
    "  transformer.layers.0.linear2.bias: [128] = 128 params\n",
    "  transformer.layers.0.norm1.weight: [128] = 128 params\n",
    "  transformer.layers.0.norm1.bias: [128] = 128 params\n",
    "  transformer.layers.0.norm2.weight: [128] = 128 params\n",
    "  transformer.layers.0.norm2.bias: [128] = 128 params\n",
    "  transformer.layers.1.self_attn.in_proj_weight: [384, 128] = 49,152 params\n",
    "  transformer.layers.1.self_attn.in_proj_bias: [384] = 384 params\n",
    "  transformer.layers.1.self_attn.out_proj.weight: [128, 128] = 16,384 params\n",
    "  transformer.layers.1.self_attn.out_proj.bias: [128] = 128 params\n",
    "  transformer.layers.1.linear1.weight: [256, 128] = 32,768 params\n",
    "  transformer.layers.1.linear1.bias: [256] = 256 params\n",
    "  transformer.layers.1.linear2.weight: [128, 256] = 32,768 params\n",
    "  transformer.layers.1.linear2.bias: [128] = 128 params\n",
    "  transformer.layers.1.norm1.weight: [128] = 128 params\n",
    "  transformer.layers.1.norm1.bias: [128] = 128 params\n",
    "  transformer.layers.1.norm2.weight: [128] = 128 params\n",
    "  transformer.layers.1.norm2.bias: [128] = 128 params\n",
    "  fc.0.weight: [64, 128] = 8,192 params\n",
    "  fc.0.bias: [64] = 64 params\n",
    "  fc.3.weight: [2, 64] = 128 params\n",
    "  fc.3.bias: [2] = 2 params\n",
    "  ──────────────────────────────────────────────────\n",
    "  Total parameters:     1,760,162\n",
    "  Trainable parameters: 1,760,162\n",
    "\n",
    "Training...\n",
    "Epoch 1/6: 100%|██████████| 63/63 [00:57<00:00,  1.10it/s, loss=0.0100, acc=0.8666]\n",
    "Epoch 2/6: 100%|██████████| 63/63 [00:59<00:00,  1.06it/s, loss=0.0008, acc=0.9955]\n",
    "Epoch 3/6: 100%|██████████| 63/63 [01:00<00:00,  1.04it/s, loss=0.0007, acc=0.9970]\n",
    "Epoch 4/6: 100%|██████████| 63/63 [01:00<00:00,  1.05it/s, loss=0.0006, acc=0.9970]\n",
    "Epoch 5/6: 100%|██████████| 63/63 [00:59<00:00,  1.05it/s, loss=0.0006, acc=0.9919]\n",
    "Epoch 6/6: 100%|██████████| 63/63 [01:00<00:00,  1.05it/s, loss=0.0004, acc=0.9975]\n",
    "Training time: 357.17s\n",
    "\n",
    "Evaluating...\n",
    "Test time: 5.46s\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      benign       1.00      0.81      0.90       489\n",
    "   malicious       0.78      1.00      0.88       321\n",
    "\n",
    "    accuracy                           0.89       810\n",
    "   macro avg       0.89      0.91      0.89       810\n",
    "weighted avg       0.91      0.89      0.89       810\n",
    "\n",
    "Confusion Matrix:\n",
    "                 Pred: benign  Pred: malicious\n",
    "True: benign              398               91\n",
    "True: malicious             0              321\n",
    "\n",
    "Detection Rate: 1.0000\n",
    "False Positive Rate: 0.1861\n",
    "F1-score (weighted): 0.8889\n",
    "\n",
    "============================================================\n",
    "EXPERIMENT: Window Size = 1000\n",
    "============================================================\n",
    "Train samples: 1986, Test samples: 810\n",
    "\n",
    "--- Input Shapes ---\n",
    "  Syscall input shape:   (batch_size, 1000)\n",
    "  Retval input shape:    (batch_size, 1000)\n",
    "  Param emb input shape: (batch_size, 1000, 384)\n",
    "  After embedding concat: (batch_size, 1000, 448)\n",
    "\n",
    "--- Model Architecture & Parameters ---\n",
    "  pos_encoding: [1, 1000, 128] = 128,000 params\n",
    "  syscall_embedding.weight: [81, 32] = 2,592 params\n",
    "  retval_embedding.weight: [42586, 32] = 1,362,752 params\n",
    "  input_projection.weight: [128, 448] = 57,344 params\n",
    "  input_projection.bias: [128] = 128 params\n",
    "  transformer.layers.0.self_attn.in_proj_weight: [384, 128] = 49,152 params\n",
    "  transformer.layers.0.self_attn.in_proj_bias: [384] = 384 params\n",
    "  transformer.layers.0.self_attn.out_proj.weight: [128, 128] = 16,384 params\n",
    "  transformer.layers.0.self_attn.out_proj.bias: [128] = 128 params\n",
    "  transformer.layers.0.linear1.weight: [256, 128] = 32,768 params\n",
    "  transformer.layers.0.linear1.bias: [256] = 256 params\n",
    "  transformer.layers.0.linear2.weight: [128, 256] = 32,768 params\n",
    "  transformer.layers.0.linear2.bias: [128] = 128 params\n",
    "  transformer.layers.0.norm1.weight: [128] = 128 params\n",
    "  transformer.layers.0.norm1.bias: [128] = 128 params\n",
    "  transformer.layers.0.norm2.weight: [128] = 128 params\n",
    "  transformer.layers.0.norm2.bias: [128] = 128 params\n",
    "  transformer.layers.1.self_attn.in_proj_weight: [384, 128] = 49,152 params\n",
    "  transformer.layers.1.self_attn.in_proj_bias: [384] = 384 params\n",
    "  transformer.layers.1.self_attn.out_proj.weight: [128, 128] = 16,384 params\n",
    "  transformer.layers.1.self_attn.out_proj.bias: [128] = 128 params\n",
    "  transformer.layers.1.linear1.weight: [256, 128] = 32,768 params\n",
    "  transformer.layers.1.linear1.bias: [256] = 256 params\n",
    "  transformer.layers.1.linear2.weight: [128, 256] = 32,768 params\n",
    "  transformer.layers.1.linear2.bias: [128] = 128 params\n",
    "  transformer.layers.1.norm1.weight: [128] = 128 params\n",
    "  transformer.layers.1.norm1.bias: [128] = 128 params\n",
    "  transformer.layers.1.norm2.weight: [128] = 128 params\n",
    "  transformer.layers.1.norm2.bias: [128] = 128 params\n",
    "  fc.0.weight: [64, 128] = 8,192 params\n",
    "  fc.0.bias: [64] = 64 params\n",
    "  fc.3.weight: [2, 64] = 128 params\n",
    "  fc.3.bias: [2] = 2 params\n",
    "  ──────────────────────────────────────────────────\n",
    "  Total parameters:     1,824,162\n",
    "  Trainable parameters: 1,824,162\n",
    "\n",
    "Training...\n",
    "Epoch 1/6: 100%|██████████| 63/63 [03:12<00:00,  3.06s/it, loss=0.0081, acc=0.9028]\n",
    "Epoch 2/6: 100%|██████████| 63/63 [03:22<00:00,  3.21s/it, loss=0.0035, acc=0.9708]\n",
    "Epoch 3/6: 100%|██████████| 63/63 [03:33<00:00,  3.39s/it, loss=0.0048, acc=0.9653]\n",
    "Epoch 4/6: 100%|██████████| 63/63 [03:24<00:00,  3.25s/it, loss=0.0083, acc=0.8938]\n",
    "Epoch 5/6: 100%|██████████| 63/63 [03:26<00:00,  3.28s/it, loss=0.0004, acc=0.9985]\n",
    "Epoch 6/6: 100%|██████████| 63/63 [03:26<00:00,  3.27s/it, loss=0.0004, acc=0.9985]\n",
    "Training time: 1226.17s\n",
    "\n",
    "Evaluating...\n",
    "Test time: 11.58s\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      benign       1.00      0.81      0.90       489\n",
    "   malicious       0.78      1.00      0.87       321\n",
    "\n",
    "    accuracy                           0.89       810\n",
    "   macro avg       0.89      0.91      0.89       810\n",
    "weighted avg       0.91      0.89      0.89       810\n",
    "\n",
    "Confusion Matrix:\n",
    "                 Pred: benign  Pred: malicious\n",
    "True: benign              397               92\n",
    "True: malicious             0              321\n",
    "\n",
    "Detection Rate: 1.0000\n",
    "False Positive Rate: 0.1881\n",
    "F1-score (weighted): 0.8876\n",
    "\n",
    "============================================================\n",
    "EXPERIMENT: Window Size = 2000\n",
    "============================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS (Transformer - All Features Combined)\n",
      "================================================================================\n",
      " Window Size Detection Rate False Positive Rate F1-score (weighted) Train Time (s) Test Time (s)\n",
      "         250         0.9907              0.1779              0.8901         123.06          2.12\n",
      "         500         1.0000              0.1861              0.8889         357.17          5.46\n",
      "        1000         1.0000              0.1881              0.8876        1226.17         11.58\n",
      "        2000         1.0000              0.1881              0.8876        9956.35         25.86\n",
      "\n",
      "Features combined:\n",
      "  - Syscall embedding: 32 dims\n",
      "  - Return value embedding: 32 dims\n",
      "  - Parameter embedding (sentence transformer): 384 dims\n",
      "  - Total input: 448 dims per timestep\n",
      "  - Projected to hidden dim: 128 dims\n",
      "\n",
      "Transformer Architecture:\n",
      "  - Number of heads: 4\n",
      "  - Number of layers: 2\n",
      "  - Feed-forward dim: 256\n",
      "  - Dropout: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Summary Results Table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY OF RESULTS (Transformer - All Features Combined)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Include earlier results for 250, 500, and 1000 window sizes\n",
    "all_results = [\n",
    "    {'Window Size': 250, 'Detection Rate': 0.9907, 'False Positive Rate': 0.1779, 'F1-score (weighted)': 0.8901, 'Train Time (s)': 123.06, 'Test Time (s)': 2.12},\n",
    "    {'Window Size': 500, 'Detection Rate': 1.0000, 'False Positive Rate': 0.1861, 'F1-score (weighted)': 0.8889, 'Train Time (s)': 357.17, 'Test Time (s)': 5.46},\n",
    "    {'Window Size': 1000, 'Detection Rate': 1.0000, 'False Positive Rate': 0.1881, 'F1-score (weighted)': 0.8876, 'Train Time (s)': 1226.17, 'Test Time (s)': 11.58},\n",
    "] + results\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df['Detection Rate'] = results_df['Detection Rate'].apply(lambda x: f\"{x:.4f}\")\n",
    "results_df['False Positive Rate'] = results_df['False Positive Rate'].apply(lambda x: f\"{x:.4f}\")\n",
    "results_df['F1-score (weighted)'] = results_df['F1-score (weighted)'].apply(lambda x: f\"{x:.4f}\")\n",
    "results_df['Train Time (s)'] = results_df['Train Time (s)'].apply(lambda x: f\"{x:.2f}\")\n",
    "results_df['Test Time (s)'] = results_df['Test Time (s)'].apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nFeatures combined:\")\n",
    "print(f\"  - Syscall embedding: {SYSCALL_EMBED_DIM} dims\")\n",
    "print(f\"  - Return value embedding: {RETVAL_EMBED_DIM} dims\")\n",
    "print(f\"  - Parameter embedding (sentence transformer): {PARAM_EMBED_DIM} dims\")\n",
    "print(f\"  - Total input: {SYSCALL_EMBED_DIM + RETVAL_EMBED_DIM + PARAM_EMBED_DIM} dims per timestep\")\n",
    "print(f\"  - Projected to hidden dim: 128 dims\")\n",
    "print(f\"\\nTransformer Architecture:\")\n",
    "print(f\"  - Number of heads: 4\")\n",
    "print(f\"  - Number of layers: 2\")\n",
    "print(f\"  - Feed-forward dim: 256\")\n",
    "print(f\"  - Dropout: 0.1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
