{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1e4505d1",
      "metadata": {},
      "source": [
        "## Feature: Syscall only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a84940df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost - Syscall Classification\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../../configs')\n",
        "from config_loader import get_split_with_labels\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Config\n",
        "SPLIT = '70'\n",
        "WINDOW_SIZES = [250, 500, 1000, 2000]  # Different sliding window lengths to test\n",
        "\n",
        "print(\"XGBoost - Syscall Classification\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "426ce138",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train files: 21, Test files: 9\n",
            "\n",
            "Training set:\n",
            "  Total runs: 1986\n",
            "  Benign runs: 1484\n",
            "  Malicious runs: 502\n",
            "\n",
            "Test set:\n",
            "  Total runs: 810\n",
            "  Benign runs: 489\n",
            "  Malicious runs: 321\n",
            "\n",
            "Vocabulary size: 80\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "train_files, test_files = get_split_with_labels(SPLIT)\n",
        "print(f\"Train files: {len(train_files)}, Test files: {len(test_files)}\")\n",
        "\n",
        "def load_runs(file_path):\n",
        "    \"\"\"Load syscalls grouped by run.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    runs = df.groupby('run')['syscall'].apply(list).tolist()\n",
        "    return runs\n",
        "\n",
        "# Count runs per label for training and test sets\n",
        "def count_runs_per_label(file_label_pairs):\n",
        "    \"\"\"Count total runs per label.\"\"\"\n",
        "    counts = {'benign': 0, 'malicious': 0}\n",
        "    for path, label in file_label_pairs:\n",
        "        runs = load_runs(path)\n",
        "        counts[label] += len(runs)\n",
        "    return counts\n",
        "\n",
        "train_counts = count_runs_per_label(train_files)\n",
        "test_counts = count_runs_per_label(test_files)\n",
        "\n",
        "print(f\"\\nTraining set:\")\n",
        "print(f\"  Total runs: {sum(train_counts.values())}\")\n",
        "print(f\"  Benign runs: {train_counts['benign']}\")\n",
        "print(f\"  Malicious runs: {train_counts['malicious']}\")\n",
        "\n",
        "print(f\"\\nTest set:\")\n",
        "print(f\"  Total runs: {sum(test_counts.values())}\")\n",
        "print(f\"  Benign runs: {test_counts['benign']}\")\n",
        "print(f\"  Malicious runs: {test_counts['malicious']}\")\n",
        "\n",
        "# Collect all syscalls for building vocabulary\n",
        "all_syscalls = []\n",
        "for path, _ in train_files + test_files:\n",
        "    for run in load_runs(path):\n",
        "        all_syscalls.extend(run)\n",
        "\n",
        "# Build syscall encoder\n",
        "syscall_encoder = LabelEncoder()\n",
        "syscall_encoder.fit(all_syscalls)\n",
        "vocab_size = len(syscall_encoder.classes_)\n",
        "print(f\"\\nVocabulary size: {vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0e7bc286",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using syscall frequency histogram as features\n"
          ]
        }
      ],
      "source": [
        "def extract_frequency_features(syscalls, encoder, max_len):\n",
        "    \"\"\"Extract syscall frequency features from a sequence.\"\"\"\n",
        "    # Truncate sequence to max_len\n",
        "    syscalls = syscalls[:max_len]\n",
        "    \n",
        "    # Encode syscalls\n",
        "    encoded = encoder.transform(syscalls)\n",
        "    \n",
        "    # Count frequencies\n",
        "    vocab_size = len(encoder.classes_)\n",
        "    freq = np.zeros(vocab_size)\n",
        "    for idx in encoded:\n",
        "        freq[idx] += 1\n",
        "    \n",
        "    # Normalize by total count\n",
        "    total = len(encoded)\n",
        "    if total > 0:\n",
        "        freq = freq / total\n",
        "    \n",
        "    return freq\n",
        "\n",
        "def prepare_dataset(file_label_pairs, encoder, max_len):\n",
        "    \"\"\"Prepare feature matrix and labels from file-label pairs.\"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    label_map = {'benign': 0, 'malicious': 1}\n",
        "    \n",
        "    for path, label in file_label_pairs:\n",
        "        runs = load_runs(path)\n",
        "        for run_syscalls in runs:\n",
        "            if len(run_syscalls) > 0:  # Need at least 1 syscall\n",
        "                features = extract_frequency_features(run_syscalls, encoder, max_len)\n",
        "                X.append(features)\n",
        "                y.append(label_map[label])\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "print(f\"Using syscall frequency histogram as features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fd68a8e2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EXPERIMENT: Window Size = 250\n",
            "============================================================\n",
            "Train samples: 1986, Test samples: 810\n",
            "Feature dimension: 80\n",
            "\n",
            "Training XGBoost...\n",
            "Training time: 0.36s\n",
            "\n",
            "Evaluating...\n",
            "Test time: 0.01s\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       0.99      0.82      0.90       489\n",
            "   malicious       0.79      0.99      0.88       321\n",
            "\n",
            "    accuracy                           0.89       810\n",
            "   macro avg       0.89      0.91      0.89       810\n",
            "weighted avg       0.91      0.89      0.89       810\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred: benign  Pred: malicious\n",
            "True: benign              402               87\n",
            "True: malicious             3              318\n",
            "\n",
            "Detection Rate: 0.9907\n",
            "False Positive Rate: 0.1779\n",
            "F1-score (weighted): 0.8901\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT: Window Size = 500\n",
            "============================================================\n",
            "Train samples: 1986, Test samples: 810\n",
            "Feature dimension: 80\n",
            "\n",
            "Training XGBoost...\n",
            "Training time: 0.14s\n",
            "\n",
            "Evaluating...\n",
            "Test time: 0.00s\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       1.00      0.81      0.90       489\n",
            "   malicious       0.78      1.00      0.87       321\n",
            "\n",
            "    accuracy                           0.89       810\n",
            "   macro avg       0.89      0.91      0.89       810\n",
            "weighted avg       0.91      0.89      0.89       810\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred: benign  Pred: malicious\n",
            "True: benign              398               91\n",
            "True: malicious             1              320\n",
            "\n",
            "Detection Rate: 0.9969\n",
            "False Positive Rate: 0.1861\n",
            "F1-score (weighted): 0.8876\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT: Window Size = 1000\n",
            "============================================================\n",
            "Train samples: 1986, Test samples: 810\n",
            "Feature dimension: 80\n",
            "\n",
            "Training XGBoost...\n",
            "Training time: 0.16s\n",
            "\n",
            "Evaluating...\n",
            "Test time: 0.00s\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       1.00      0.81      0.90       489\n",
            "   malicious       0.78      1.00      0.88       321\n",
            "\n",
            "    accuracy                           0.89       810\n",
            "   macro avg       0.89      0.91      0.89       810\n",
            "weighted avg       0.91      0.89      0.89       810\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred: benign  Pred: malicious\n",
            "True: benign              398               91\n",
            "True: malicious             0              321\n",
            "\n",
            "Detection Rate: 1.0000\n",
            "False Positive Rate: 0.1861\n",
            "F1-score (weighted): 0.8889\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT: Window Size = 2000\n",
            "============================================================\n",
            "Train samples: 1986, Test samples: 810\n",
            "Feature dimension: 80\n",
            "\n",
            "Training XGBoost...\n",
            "Training time: 0.24s\n",
            "\n",
            "Evaluating...\n",
            "Test time: 0.00s\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       0.82      0.81      0.82       489\n",
            "   malicious       0.72      0.74      0.73       321\n",
            "\n",
            "    accuracy                           0.78       810\n",
            "   macro avg       0.77      0.77      0.77       810\n",
            "weighted avg       0.78      0.78      0.78       810\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred: benign  Pred: malicious\n",
            "True: benign              398               91\n",
            "True: malicious            85              236\n",
            "\n",
            "Detection Rate: 0.7352\n",
            "False Positive Rate: 0.1861\n",
            "F1-score (weighted): 0.7831\n"
          ]
        }
      ],
      "source": [
        "# Run experiments with different window sizes\n",
        "results = []\n",
        "\n",
        "for window_size in WINDOW_SIZES:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXPERIMENT: Window Size = {window_size}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Prepare datasets\n",
        "    X_train, y_train = prepare_dataset(train_files, syscall_encoder, window_size)\n",
        "    X_test, y_test = prepare_dataset(test_files, syscall_encoder, window_size)\n",
        "    \n",
        "    print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
        "    print(f\"Feature dimension: {X_train.shape[1]}\")\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Create XGBoost model\n",
        "    model = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss'\n",
        "    )\n",
        "    \n",
        "    # Training\n",
        "    print(f\"\\nTraining XGBoost...\")\n",
        "    train_start_time = time.time()\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    train_time = time.time() - train_start_time\n",
        "    print(f\"Training time: {train_time:.2f}s\")\n",
        "    \n",
        "    # Testing\n",
        "    print(f\"\\nEvaluating...\")\n",
        "    test_start_time = time.time()\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    test_time = time.time() - test_start_time\n",
        "    print(f\"Test time: {test_time:.2f}s\")\n",
        "    \n",
        "    # Calculate metrics\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
        "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "    # Store results\n",
        "    results.append({\n",
        "        'Window Size': window_size,\n",
        "        'Detection Rate': detection_rate,\n",
        "        'False Positive Rate': false_positive_rate,\n",
        "        'F1-score (weighted)': f1_weighted,\n",
        "        'Train Time (s)': train_time,\n",
        "        'Test Time (s)': test_time\n",
        "    })\n",
        "    \n",
        "    # Print detailed results\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['benign', 'malicious']))\n",
        "    \n",
        "    labels = ['benign', 'malicious']\n",
        "    cm_df = pd.DataFrame(cm, index=[f'True: {l}' for l in labels], columns=[f'Pred: {l}' for l in labels])\n",
        "    print(f\"Confusion Matrix:\")\n",
        "    print(cm_df)\n",
        "    \n",
        "    print(f\"\\nDetection Rate: {detection_rate:.4f}\")\n",
        "    print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n",
        "    print(f\"F1-score (weighted): {f1_weighted:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d66bcf7e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "SUMMARY OF RESULTS\n",
            "================================================================================\n",
            " Window Size Detection Rate False Positive Rate F1-score (weighted) Train Time (s) Test Time (s)\n",
            "         250         0.9907              0.1779              0.8901           0.36          0.01\n",
            "         500         0.9969              0.1861              0.8876           0.14          0.00\n",
            "        1000         1.0000              0.1861              0.8889           0.16          0.00\n",
            "        2000         0.7352              0.1861              0.7831           0.24          0.00\n"
          ]
        }
      ],
      "source": [
        "# Summary Results Table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY OF RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df['Detection Rate'] = results_df['Detection Rate'].apply(lambda x: f\"{x:.4f}\")\n",
        "results_df['False Positive Rate'] = results_df['False Positive Rate'].apply(lambda x: f\"{x:.4f}\")\n",
        "results_df['F1-score (weighted)'] = results_df['F1-score (weighted)'].apply(lambda x: f\"{x:.4f}\")\n",
        "results_df['Train Time (s)'] = results_df['Train Time (s)'].apply(lambda x: f\"{x:.2f}\")\n",
        "results_df['Test Time (s)'] = results_df['Test Time (s)'].apply(lambda x: f\"{x:.2f}\")\n",
        "\n",
        "print(results_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38232818",
      "metadata": {},
      "source": [
        "## Feature: Return values only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1d0d2fe8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost - Return Value Classification\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../../configs')\n",
        "from config_loader import get_split_with_labels\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Config\n",
        "SPLIT = '70'\n",
        "WINDOW_SIZES = [250, 500, 1000, 2000]  # Different sliding window lengths to test\n",
        "\n",
        "print(\"XGBoost - Return Value Classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "806ed9bd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train files: 21, Test files: 9\n",
            "\n",
            "Training set:\n",
            "  Total runs: 1986\n",
            "  Benign runs: 1484\n",
            "  Malicious runs: 502\n",
            "\n",
            "Test set:\n",
            "  Total runs: 810\n",
            "  Benign runs: 489\n",
            "  Malicious runs: 321\n",
            "\n",
            "Vocabulary size: 42585\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "train_files, test_files = get_split_with_labels(SPLIT)\n",
        "print(f\"Train files: {len(train_files)}, Test files: {len(test_files)}\")\n",
        "\n",
        "def load_runs_retval(file_path):\n",
        "    \"\"\"Load return values grouped by run.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    runs = df.groupby('run')['Ret'].apply(list).tolist()\n",
        "    return runs\n",
        "\n",
        "# Count runs per label for training and test sets\n",
        "def count_runs_per_label(file_label_pairs):\n",
        "    \"\"\"Count total runs per label.\"\"\"\n",
        "    counts = {'benign': 0, 'malicious': 0}\n",
        "    for path, label in file_label_pairs:\n",
        "        runs = load_runs_retval(path)\n",
        "        counts[label] += len(runs)\n",
        "    return counts\n",
        "\n",
        "train_counts = count_runs_per_label(train_files)\n",
        "test_counts = count_runs_per_label(test_files)\n",
        "\n",
        "print(f\"\\nTraining set:\")\n",
        "print(f\"  Total runs: {sum(train_counts.values())}\")\n",
        "print(f\"  Benign runs: {train_counts['benign']}\")\n",
        "print(f\"  Malicious runs: {train_counts['malicious']}\")\n",
        "\n",
        "print(f\"\\nTest set:\")\n",
        "print(f\"  Total runs: {sum(test_counts.values())}\")\n",
        "print(f\"  Benign runs: {test_counts['benign']}\")\n",
        "print(f\"  Malicious runs: {test_counts['malicious']}\")\n",
        "\n",
        "# Collect all return values for building vocabulary\n",
        "all_retvals = []\n",
        "for path, _ in train_files + test_files:\n",
        "    for run in load_runs_retval(path):\n",
        "        all_retvals.extend(run)\n",
        "\n",
        "# Build return value encoder\n",
        "retval_encoder = LabelEncoder()\n",
        "retval_encoder.fit(all_retvals)\n",
        "vocab_size = len(retval_encoder.classes_)\n",
        "print(f\"\\nVocabulary size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0f30a49c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using return value frequency histogram as features\n"
          ]
        }
      ],
      "source": [
        "def extract_frequency_features(retvals, encoder, max_len):\n",
        "    \"\"\"Extract return value frequency features from a sequence.\"\"\"\n",
        "    # Truncate sequence to max_len\n",
        "    retvals = retvals[:max_len]\n",
        "    \n",
        "    # Encode return values\n",
        "    encoded = encoder.transform(retvals)\n",
        "    \n",
        "    # Count frequencies\n",
        "    vocab_size = len(encoder.classes_)\n",
        "    freq = np.zeros(vocab_size)\n",
        "    for idx in encoded:\n",
        "        freq[idx] += 1\n",
        "    \n",
        "    # Normalize by total count\n",
        "    total = len(encoded)\n",
        "    if total > 0:\n",
        "        freq = freq / total\n",
        "    \n",
        "    return freq\n",
        "\n",
        "def prepare_dataset(file_label_pairs, encoder, max_len):\n",
        "    \"\"\"Prepare feature matrix and labels from file-label pairs.\"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    label_map = {'benign': 0, 'malicious': 1}\n",
        "    \n",
        "    for path, label in file_label_pairs:\n",
        "        runs = load_runs_retval(path)\n",
        "        for run_retvals in runs:\n",
        "            if len(run_retvals) > 0:  # Need at least 1 return value\n",
        "                features = extract_frequency_features(run_retvals, encoder, max_len)\n",
        "                X.append(features)\n",
        "                y.append(label_map[label])\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "print(f\"Using return value frequency histogram as features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2437c184",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EXPERIMENT: Window Size = 250\n",
            "============================================================\n",
            "Train samples: 1986, Test samples: 810\n",
            "Feature dimension: 42585\n",
            "\n",
            "Training XGBoost...\n",
            "Training time: 5.66s\n",
            "\n",
            "Evaluating...\n",
            "Test time: 0.04s\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       0.99      0.82      0.90       489\n",
            "   malicious       0.79      0.99      0.88       321\n",
            "\n",
            "    accuracy                           0.89       810\n",
            "   macro avg       0.89      0.91      0.89       810\n",
            "weighted avg       0.91      0.89      0.89       810\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred: benign  Pred: malicious\n",
            "True: benign              403               86\n",
            "True: malicious             3              318\n",
            "\n",
            "Detection Rate: 0.9907\n",
            "False Positive Rate: 0.1759\n",
            "F1-score (weighted): 0.8913\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT: Window Size = 500\n",
            "============================================================\n",
            "Train samples: 1986, Test samples: 810\n",
            "Feature dimension: 42585\n",
            "\n",
            "Training XGBoost...\n",
            "Training time: 5.26s\n",
            "\n",
            "Evaluating...\n",
            "Test time: 0.04s\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       1.00      0.92      0.96       489\n",
            "   malicious       0.89      1.00      0.94       321\n",
            "\n",
            "    accuracy                           0.95       810\n",
            "   macro avg       0.94      0.96      0.95       810\n",
            "weighted avg       0.95      0.95      0.95       810\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred: benign  Pred: malicious\n",
            "True: benign              449               40\n",
            "True: malicious             1              320\n",
            "\n",
            "Detection Rate: 0.9969\n",
            "False Positive Rate: 0.0818\n",
            "F1-score (weighted): 0.9498\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT: Window Size = 1000\n",
            "============================================================\n",
            "Train samples: 1986, Test samples: 810\n",
            "Feature dimension: 42585\n",
            "\n",
            "Training XGBoost...\n",
            "Training time: 5.63s\n",
            "\n",
            "Evaluating...\n",
            "Test time: 0.04s\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       1.00      1.00      1.00       489\n",
            "   malicious       0.99      1.00      1.00       321\n",
            "\n",
            "    accuracy                           1.00       810\n",
            "   macro avg       1.00      1.00      1.00       810\n",
            "weighted avg       1.00      1.00      1.00       810\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred: benign  Pred: malicious\n",
            "True: benign              487                2\n",
            "True: malicious             0              321\n",
            "\n",
            "Detection Rate: 1.0000\n",
            "False Positive Rate: 0.0041\n",
            "F1-score (weighted): 0.9975\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT: Window Size = 2000\n",
            "============================================================\n",
            "Train samples: 1986, Test samples: 810\n",
            "Feature dimension: 42585\n",
            "\n",
            "Training XGBoost...\n",
            "Training time: 6.06s\n",
            "\n",
            "Evaluating...\n",
            "Test time: 0.04s\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       1.00      1.00      1.00       489\n",
            "   malicious       1.00      1.00      1.00       321\n",
            "\n",
            "    accuracy                           1.00       810\n",
            "   macro avg       1.00      1.00      1.00       810\n",
            "weighted avg       1.00      1.00      1.00       810\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred: benign  Pred: malicious\n",
            "True: benign              488                1\n",
            "True: malicious             0              321\n",
            "\n",
            "Detection Rate: 1.0000\n",
            "False Positive Rate: 0.0020\n",
            "F1-score (weighted): 0.9988\n"
          ]
        }
      ],
      "source": [
        "# Run experiments with different window sizes\n",
        "results = []\n",
        "\n",
        "for window_size in WINDOW_SIZES:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXPERIMENT: Window Size = {window_size}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Prepare datasets\n",
        "    X_train, y_train = prepare_dataset(train_files, retval_encoder, window_size)\n",
        "    X_test, y_test = prepare_dataset(test_files, retval_encoder, window_size)\n",
        "    \n",
        "    print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
        "    print(f\"Feature dimension: {X_train.shape[1]}\")\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Create XGBoost model\n",
        "    model = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss'\n",
        "    )\n",
        "    \n",
        "    # Training\n",
        "    print(f\"\\nTraining XGBoost...\")\n",
        "    train_start_time = time.time()\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    train_time = time.time() - train_start_time\n",
        "    print(f\"Training time: {train_time:.2f}s\")\n",
        "    \n",
        "    # Testing\n",
        "    print(f\"\\nEvaluating...\")\n",
        "    test_start_time = time.time()\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    test_time = time.time() - test_start_time\n",
        "    print(f\"Test time: {test_time:.2f}s\")\n",
        "    \n",
        "    # Calculate metrics\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
        "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "    # Store results\n",
        "    results.append({\n",
        "        'Window Size': window_size,\n",
        "        'Detection Rate': detection_rate,\n",
        "        'False Positive Rate': false_positive_rate,\n",
        "        'F1-score (weighted)': f1_weighted,\n",
        "        'Train Time (s)': train_time,\n",
        "        'Test Time (s)': test_time\n",
        "    })\n",
        "    \n",
        "    # Print detailed results\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['benign', 'malicious']))\n",
        "    \n",
        "    labels = ['benign', 'malicious']\n",
        "    cm_df = pd.DataFrame(cm, index=[f'True: {l}' for l in labels], columns=[f'Pred: {l}' for l in labels])\n",
        "    print(f\"Confusion Matrix:\")\n",
        "    print(cm_df)\n",
        "    \n",
        "    print(f\"\\nDetection Rate: {detection_rate:.4f}\")\n",
        "    print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n",
        "    print(f\"F1-score (weighted): {f1_weighted:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "84484b30",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "SUMMARY OF RESULTS\n",
            "================================================================================\n",
            " Window Size Detection Rate False Positive Rate F1-score (weighted) Train Time (s) Test Time (s)\n",
            "         250         0.9907              0.1759              0.8913           5.66          0.04\n",
            "         500         0.9969              0.0818              0.9498           5.26          0.04\n",
            "        1000         1.0000              0.0041              0.9975           5.63          0.04\n",
            "        2000         1.0000              0.0020              0.9988           6.06          0.04\n"
          ]
        }
      ],
      "source": [
        "# Summary Results Table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY OF RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df['Detection Rate'] = results_df['Detection Rate'].apply(lambda x: f\"{x:.4f}\")\n",
        "results_df['False Positive Rate'] = results_df['False Positive Rate'].apply(lambda x: f\"{x:.4f}\")\n",
        "results_df['F1-score (weighted)'] = results_df['F1-score (weighted)'].apply(lambda x: f\"{x:.4f}\")\n",
        "results_df['Train Time (s)'] = results_df['Train Time (s)'].apply(lambda x: f\"{x:.2f}\")\n",
        "results_df['Test Time (s)'] = results_df['Test Time (s)'].apply(lambda x: f\"{x:.2f}\")\n",
        "\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "555d4a35",
      "metadata": {},
      "source": [
        "## Feature: Parameters only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e945c05",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "28eff0d4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost - Parameters Classification (Sentence Embeddings)\n",
            "Loading sentence transformer model...\n",
            "Sentence embedding dimension: 384\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../../configs')\n",
        "from config_loader import get_split_with_labels\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Config\n",
        "SPLIT = '70'\n",
        "WINDOW_SIZES = [250, 500, 1000, 2000]  # Different sliding window lengths to test\n",
        "\n",
        "print(\"XGBoost - Parameters Classification (Sentence Embeddings)\")\n",
        "\n",
        "# Load sentence transformer model for semantic embeddings\n",
        "print(\"Loading sentence transformer model...\")\n",
        "sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
        "EMBEDDING_DIM = sentence_model.get_sentence_embedding_dimension()\n",
        "print(f\"Sentence embedding dimension: {EMBEDDING_DIM}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c0283997",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train files: 21, Test files: 9\n",
            "\n",
            "Training set:\n",
            "  Total runs: 1986\n",
            "  Benign runs: 1484\n",
            "  Malicious runs: 502\n",
            "\n",
            "Test set:\n",
            "  Total runs: 810\n",
            "  Benign runs: 489\n",
            "  Malicious runs: 321\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "train_files, test_files = get_split_with_labels(SPLIT)\n",
        "print(f\"Train files: {len(train_files)}, Test files: {len(test_files)}\")\n",
        "\n",
        "def load_runs_params_raw(file_path):\n",
        "    \"\"\"Load raw parameter strings grouped by run (list of param strings per run).\n",
        "    \n",
        "    Returns list of runs, where each run is a list of parameter strings (one per syscall).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    runs = []\n",
        "    for run_id, group in df.groupby('run'):\n",
        "        # Keep parameters as list of strings (one per syscall)\n",
        "        run_params = group['parameters'].tolist()\n",
        "        runs.append(run_params)\n",
        "    return runs\n",
        "\n",
        "# Count runs per label for training and test sets\n",
        "def count_runs_per_label(file_label_pairs):\n",
        "    \"\"\"Count total runs per label.\"\"\"\n",
        "    counts = {'benign': 0, 'malicious': 0}\n",
        "    for path, label in file_label_pairs:\n",
        "        runs = load_runs_params_raw(path)\n",
        "        counts[label] += len(runs)\n",
        "    return counts\n",
        "\n",
        "train_counts = count_runs_per_label(train_files)\n",
        "test_counts = count_runs_per_label(test_files)\n",
        "\n",
        "print(f\"\\nTraining set:\")\n",
        "print(f\"  Total runs: {sum(train_counts.values())}\")\n",
        "print(f\"  Benign runs: {train_counts['benign']}\")\n",
        "print(f\"  Malicious runs: {train_counts['malicious']}\")\n",
        "\n",
        "print(f\"\\nTest set:\")\n",
        "print(f\"  Total runs: {sum(test_counts.values())}\")\n",
        "print(f\"  Benign runs: {test_counts['benign']}\")\n",
        "print(f\"  Malicious runs: {test_counts['malicious']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7b5d0ad4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting unique parameter strings...\n",
            "Unique parameter strings: 266019\n",
            "Computing sentence embeddings (this may take a few minutes)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c33d196533a4f14826d995c56026a1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1040 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings computed. Shape per embedding: 384\n"
          ]
        }
      ],
      "source": [
        "# Pre-compute sentence embeddings for all unique parameter strings\n",
        "print(\"Collecting unique parameter strings...\")\n",
        "\n",
        "unique_params = set()\n",
        "for path, _ in train_files + test_files:\n",
        "    for run_params in load_runs_params_raw(path):\n",
        "        for param_str in run_params:\n",
        "            # Convert to string and handle NaN\n",
        "            if pd.isna(param_str):\n",
        "                unique_params.add('<EMPTY>')\n",
        "            else:\n",
        "                unique_params.add(str(param_str))\n",
        "\n",
        "unique_params = list(unique_params)\n",
        "print(f\"Unique parameter strings: {len(unique_params)}\")\n",
        "\n",
        "# Compute embeddings for all unique strings in batches\n",
        "print(\"Computing sentence embeddings (this may take a few minutes)...\")\n",
        "param_embeddings = sentence_model.encode(\n",
        "    unique_params, \n",
        "    show_progress_bar=True, \n",
        "    batch_size=256,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "\n",
        "# Create a mapping from parameter string to embedding\n",
        "param_to_embedding = {param: emb for param, emb in zip(unique_params, param_embeddings)}\n",
        "print(f\"Embeddings computed. Shape per embedding: {EMBEDDING_DIM}\")\n",
        "\n",
        "# Create a zero embedding for padding\n",
        "PAD_EMBEDDING = np.zeros(EMBEDDING_DIM, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9b7dffb1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using sentence embeddings with mean pooling as features\n",
            "Feature dimension: 384\n"
          ]
        }
      ],
      "source": [
        "def extract_sentence_embedding_features(run_params, param_to_embedding, pad_embedding, window_size):\n",
        "    \"\"\"Extract aggregated sentence embedding features from a run.\n",
        "    \n",
        "    Uses mean pooling over all parameter embeddings in the window to create\n",
        "    a fixed-size feature vector (384 dimensions).\n",
        "    \n",
        "    Args:\n",
        "        run_params: List of parameter strings (one per syscall) for a run\n",
        "        param_to_embedding: Dict mapping parameter strings to embeddings\n",
        "        pad_embedding: Zero embedding for missing/empty parameters\n",
        "        window_size: Maximum number of syscalls to consider\n",
        "    \n",
        "    Returns:\n",
        "        384-dimensional feature vector (mean of all embeddings)\n",
        "    \"\"\"\n",
        "    # Take first window_size syscalls\n",
        "    params_to_use = run_params[:window_size]\n",
        "    \n",
        "    embeddings = []\n",
        "    for param_str in params_to_use:\n",
        "        if pd.isna(param_str):\n",
        "            key = '<EMPTY>'\n",
        "        else:\n",
        "            key = str(param_str)\n",
        "        embeddings.append(param_to_embedding.get(key, pad_embedding))\n",
        "    \n",
        "    if embeddings:\n",
        "        embeddings = np.array(embeddings)\n",
        "        # Mean pooling across all syscall parameter embeddings\n",
        "        return embeddings.mean(axis=0)  # Returns 384-dim vector\n",
        "    return pad_embedding\n",
        "\n",
        "def prepare_dataset_params(file_label_pairs, param_to_embedding, pad_embedding, window_size):\n",
        "    \"\"\"Prepare feature matrix and labels using sentence embeddings.\"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    label_map = {'benign': 0, 'malicious': 1}\n",
        "    \n",
        "    for path, label in file_label_pairs:\n",
        "        runs = load_runs_params_raw(path)\n",
        "        for run_params in runs:\n",
        "            if len(run_params) > 0:  # Need at least 1 syscall\n",
        "                features = extract_sentence_embedding_features(\n",
        "                    run_params, param_to_embedding, pad_embedding, window_size\n",
        "                )\n",
        "                X.append(features)\n",
        "                y.append(label_map[label])\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "print(f\"Using sentence embeddings with mean pooling as features\")\n",
        "print(f\"Feature dimension: {EMBEDDING_DIM}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d71e5d2d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EXPERIMENT: Window Size = 250\n",
            "============================================================\n",
            "Train samples: 1986, Test samples: 810\n",
            "Feature dimension: 384\n",
            "\n",
            "Training XGBoost...\n",
            "Training time: 5.83s\n",
            "\n",
            "Evaluating...\n",
            "Test time: 0.02s\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       0.98      0.82      0.89       489\n",
            "   malicious       0.78      0.98      0.87       321\n",
            "\n",
            "    accuracy                           0.88       810\n",
            "   macro avg       0.88      0.90      0.88       810\n",
            "weighted avg       0.90      0.88      0.88       810\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred: benign  Pred: malicious\n",
            "True: benign              402               87\n",
            "True: malicious             8              313\n",
            "\n",
            "Detection Rate: 0.9751\n",
            "False Positive Rate: 0.1779\n",
            "F1-score (weighted): 0.8840\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT: Window Size = 500\n",
            "============================================================\n",
            "Train samples: 1986, Test samples: 810\n",
            "Feature dimension: 384\n",
            "\n",
            "Training XGBoost...\n",
            "Training time: 4.92s\n",
            "\n",
            "Evaluating...\n",
            "Test time: 0.01s\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       0.99      0.81      0.89       489\n",
            "   malicious       0.78      0.99      0.87       321\n",
            "\n",
            "    accuracy                           0.88       810\n",
            "   macro avg       0.88      0.90      0.88       810\n",
            "weighted avg       0.91      0.88      0.88       810\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred: benign  Pred: malicious\n",
            "True: benign              397               92\n",
            "True: malicious             3              318\n",
            "\n",
            "Detection Rate: 0.9907\n",
            "False Positive Rate: 0.1881\n",
            "F1-score (weighted): 0.8840\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT: Window Size = 1000\n",
            "============================================================\n",
            "Train samples: 1986, Test samples: 810\n",
            "Feature dimension: 384\n",
            "\n",
            "Training XGBoost...\n",
            "Training time: 4.68s\n",
            "\n",
            "Evaluating...\n",
            "Test time: 0.01s\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       0.99      0.81      0.89       489\n",
            "   malicious       0.78      0.99      0.87       321\n",
            "\n",
            "    accuracy                           0.88       810\n",
            "   macro avg       0.89      0.90      0.88       810\n",
            "weighted avg       0.91      0.88      0.89       810\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred: benign  Pred: malicious\n",
            "True: benign              398               91\n",
            "True: malicious             3              318\n",
            "\n",
            "Detection Rate: 0.9907\n",
            "False Positive Rate: 0.1861\n",
            "F1-score (weighted): 0.8852\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT: Window Size = 2000\n",
            "============================================================\n",
            "Train samples: 1986, Test samples: 810\n",
            "Feature dimension: 384\n",
            "\n",
            "Training XGBoost...\n",
            "Training time: 4.57s\n",
            "\n",
            "Evaluating...\n",
            "Test time: 0.01s\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       0.99      0.81      0.89       489\n",
            "   malicious       0.78      0.99      0.87       321\n",
            "\n",
            "    accuracy                           0.88       810\n",
            "   macro avg       0.89      0.90      0.88       810\n",
            "weighted avg       0.91      0.88      0.89       810\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred: benign  Pred: malicious\n",
            "True: benign              397               92\n",
            "True: malicious             2              319\n",
            "\n",
            "Detection Rate: 0.9938\n",
            "False Positive Rate: 0.1881\n",
            "F1-score (weighted): 0.8852\n"
          ]
        }
      ],
      "source": [
        "# Run experiments with different window sizes\n",
        "results = []\n",
        "\n",
        "for window_size in WINDOW_SIZES:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXPERIMENT: Window Size = {window_size}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Prepare datasets\n",
        "    X_train, y_train = prepare_dataset_params(train_files, param_to_embedding, PAD_EMBEDDING, window_size)\n",
        "    X_test, y_test = prepare_dataset_params(test_files, param_to_embedding, PAD_EMBEDDING, window_size)\n",
        "    \n",
        "    print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
        "    print(f\"Feature dimension: {X_train.shape[1]}\")\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Create XGBoost model\n",
        "    model = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss'\n",
        "    )\n",
        "    \n",
        "    # Training\n",
        "    print(f\"\\nTraining XGBoost...\")\n",
        "    train_start_time = time.time()\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    train_time = time.time() - train_start_time\n",
        "    print(f\"Training time: {train_time:.2f}s\")\n",
        "    \n",
        "    # Testing\n",
        "    print(f\"\\nEvaluating...\")\n",
        "    test_start_time = time.time()\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    test_time = time.time() - test_start_time\n",
        "    print(f\"Test time: {test_time:.2f}s\")\n",
        "    \n",
        "    # Calculate metrics\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
        "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "    # Store results\n",
        "    results.append({\n",
        "        'Window Size': window_size,\n",
        "        'Detection Rate': detection_rate,\n",
        "        'False Positive Rate': false_positive_rate,\n",
        "        'F1-score (weighted)': f1_weighted,\n",
        "        'Train Time (s)': train_time,\n",
        "        'Test Time (s)': test_time\n",
        "    })\n",
        "    \n",
        "    # Print detailed results\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['benign', 'malicious']))\n",
        "    \n",
        "    labels = ['benign', 'malicious']\n",
        "    cm_df = pd.DataFrame(cm, index=[f'True: {l}' for l in labels], columns=[f'Pred: {l}' for l in labels])\n",
        "    print(f\"Confusion Matrix:\")\n",
        "    print(cm_df)\n",
        "    \n",
        "    print(f\"\\nDetection Rate: {detection_rate:.4f}\")\n",
        "    print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n",
        "    print(f\"F1-score (weighted): {f1_weighted:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f5881e05",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "SUMMARY OF RESULTS (Sentence Embeddings)\n",
            "================================================================================\n",
            " Window Size Detection Rate False Positive Rate F1-score (weighted) Train Time (s) Test Time (s)\n",
            "         250         0.9751              0.1779              0.8840           5.83          0.02\n",
            "         500         0.9907              0.1881              0.8840           4.92          0.01\n",
            "        1000         0.9907              0.1861              0.8852           4.68          0.01\n",
            "        2000         0.9938              0.1881              0.8852           4.57          0.01\n",
            "\n",
            "Note: Using sentence embeddings reduces feature dimension from\n",
            "131k+ token frequencies to just 384 semantic embedding dimensions.\n"
          ]
        }
      ],
      "source": [
        "# Summary Results Table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY OF RESULTS (Sentence Embeddings)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df['Detection Rate'] = results_df['Detection Rate'].apply(lambda x: f\"{x:.4f}\")\n",
        "results_df['False Positive Rate'] = results_df['False Positive Rate'].apply(lambda x: f\"{x:.4f}\")\n",
        "results_df['F1-score (weighted)'] = results_df['F1-score (weighted)'].apply(lambda x: f\"{x:.4f}\")\n",
        "results_df['Train Time (s)'] = results_df['Train Time (s)'].apply(lambda x: f\"{x:.2f}\")\n",
        "results_df['Test Time (s)'] = results_df['Test Time (s)'].apply(lambda x: f\"{x:.2f}\")\n",
        "\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(\"\\nNote: Using sentence embeddings reduces feature dimension from\")\n",
        "print(\"131k+ token frequencies to just 384 semantic embedding dimensions.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
