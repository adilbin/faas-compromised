{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d7190ea8",
      "metadata": {},
      "source": [
        "## Feature: Syscalls + Return values + Parameters (Combined)\n",
        "\n",
        "Using XGBoost with combined features:\n",
        "- Syscall frequency histogram (80 dimensions)\n",
        "- Return values via feature hashing (256 dimensions)\n",
        "- Parameters via sentence embeddings (384 dimensions)\n",
        "- **Total: 720 dimensions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e65df671",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/adil-bb/anaconda3/envs/pt-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost - Combined Features Classification\n",
            "Features: Syscalls + Return Values (hashed) + Parameters (Sentence Embeddings)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../../configs')\n",
        "from config_loader import get_split_with_labels\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Config\n",
        "SPLIT = '70'\n",
        "WINDOW_SIZES = [250, 500, 1000, 2000]  # Different sliding window lengths to test\n",
        "RETVAL_HASH_DIM = 256  # Reduced dimension for return values via feature hashing\n",
        "\n",
        "print(\"XGBoost - Combined Features Classification\")\n",
        "print(\"Features: Syscalls + Return Values (hashed) + Parameters (Sentence Embeddings)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a008091d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train files: 21, Test files: 9\n",
            "\n",
            "Training set:\n",
            "  Total runs: 1986\n",
            "  Benign runs: 1484\n",
            "  Malicious runs: 502\n",
            "\n",
            "Test set:\n",
            "  Total runs: 810\n",
            "  Benign runs: 489\n",
            "  Malicious runs: 321\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "train_files, test_files = get_split_with_labels(SPLIT)\n",
        "print(f\"Train files: {len(train_files)}, Test files: {len(test_files)}\")\n",
        "\n",
        "def load_runs_combined(file_path):\n",
        "    \"\"\"Load syscalls, return values, and parameters grouped by run.\n",
        "    \n",
        "    Returns:\n",
        "        List of tuples: [(syscalls_list, retvals_list, params_list), ...]\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    runs = []\n",
        "    for run_id, group in df.groupby('run'):\n",
        "        syscalls = group['syscall'].tolist()\n",
        "        retvals = group['Ret'].tolist()\n",
        "        params = group['parameters'].tolist()\n",
        "        runs.append((syscalls, retvals, params))\n",
        "    return runs\n",
        "\n",
        "# Count runs per label\n",
        "def count_runs_per_label(file_label_pairs):\n",
        "    \"\"\"Count total runs per label.\"\"\"\n",
        "    counts = {'benign': 0, 'malicious': 0}\n",
        "    for path, label in file_label_pairs:\n",
        "        runs = load_runs_combined(path)\n",
        "        counts[label] += len(runs)\n",
        "    return counts\n",
        "\n",
        "train_counts = count_runs_per_label(train_files)\n",
        "test_counts = count_runs_per_label(test_files)\n",
        "\n",
        "print(f\"\\nTraining set:\")\n",
        "print(f\"  Total runs: {sum(train_counts.values())}\")\n",
        "print(f\"  Benign runs: {train_counts['benign']}\")\n",
        "print(f\"  Malicious runs: {train_counts['malicious']}\")\n",
        "\n",
        "print(f\"\\nTest set:\")\n",
        "print(f\"  Total runs: {sum(test_counts.values())}\")\n",
        "print(f\"  Benign runs: {test_counts['benign']}\")\n",
        "print(f\"  Malicious runs: {test_counts['malicious']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "vocab_build",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building vocabularies...\n",
            "Syscall vocabulary size: 80\n",
            "Return value dimension (feature hashing): 256\n"
          ]
        }
      ],
      "source": [
        "# Build vocabulary for syscalls (return values will use feature hashing)\n",
        "print(\"Building vocabularies...\")\n",
        "\n",
        "all_syscalls = []\n",
        "\n",
        "for path, _ in train_files + test_files:\n",
        "    for syscalls, _, _ in load_runs_combined(path):\n",
        "        all_syscalls.extend(syscalls)\n",
        "\n",
        "# Build syscall encoder\n",
        "syscall_encoder = LabelEncoder()\n",
        "syscall_encoder.fit(all_syscalls)\n",
        "syscall_vocab_size = len(syscall_encoder.classes_)\n",
        "print(f\"Syscall vocabulary size: {syscall_vocab_size}\")\n",
        "\n",
        "# Return values use feature hashing to reduce dimensionality\n",
        "print(f\"Return value dimension (feature hashing): {RETVAL_HASH_DIM}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "sentence_embed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading sentence transformer model...\n",
            "Sentence embedding dimension: 384\n",
            "\n",
            "Collecting unique parameter strings...\n",
            "Unique parameter strings: 266019\n",
            "Computing sentence embeddings (this may take a few minutes)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 1040/1040 [19:28<00:00,  1.12s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings computed successfully.\n"
          ]
        }
      ],
      "source": [
        "# Load sentence transformer and pre-compute parameter embeddings\n",
        "print(\"\\nLoading sentence transformer model...\")\n",
        "sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
        "EMBEDDING_DIM = sentence_model.get_sentence_embedding_dimension()\n",
        "print(f\"Sentence embedding dimension: {EMBEDDING_DIM}\")\n",
        "\n",
        "# Collect unique parameter strings\n",
        "print(\"\\nCollecting unique parameter strings...\")\n",
        "unique_params = set()\n",
        "for path, _ in train_files + test_files:\n",
        "    for _, _, params in load_runs_combined(path):\n",
        "        for param_str in params:\n",
        "            if pd.isna(param_str):\n",
        "                unique_params.add('<EMPTY>')\n",
        "            else:\n",
        "                unique_params.add(str(param_str))\n",
        "\n",
        "unique_params = list(unique_params)\n",
        "print(f\"Unique parameter strings: {len(unique_params)}\")\n",
        "\n",
        "# Compute embeddings for all unique strings\n",
        "print(\"Computing sentence embeddings (this may take a few minutes)...\")\n",
        "param_embeddings = sentence_model.encode(\n",
        "    unique_params, \n",
        "    show_progress_bar=True, \n",
        "    batch_size=256,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "\n",
        "# Create mapping from parameter string to embedding\n",
        "param_to_embedding = {param: emb for param, emb in zip(unique_params, param_embeddings)}\n",
        "PAD_EMBEDDING = np.zeros(EMBEDDING_DIM, dtype=np.float32)\n",
        "print(f\"Embeddings computed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "feature_extraction",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature extraction functions defined.\n",
            "Combined feature dimensions: 80 (syscall) + 256 (retval hashed) + 384 (params)\n",
            "Total feature dimension: 720\n"
          ]
        }
      ],
      "source": [
        "def extract_syscall_frequency(syscalls, encoder, max_len):\n",
        "    \"\"\"Extract syscall frequency histogram features.\"\"\"\n",
        "    syscalls = syscalls[:max_len]\n",
        "    encoded = encoder.transform(syscalls)\n",
        "    vocab_size = len(encoder.classes_)\n",
        "    freq = np.zeros(vocab_size)\n",
        "    for idx in encoded:\n",
        "        freq[idx] += 1\n",
        "    total = len(encoded)\n",
        "    if total > 0:\n",
        "        freq = freq / total\n",
        "    return freq\n",
        "\n",
        "def extract_retval_hashed(retvals, hash_dim, max_len):\n",
        "    \"\"\"Extract return value features using feature hashing.\n",
        "    \n",
        "    Uses a hash function to map return values to a fixed-size vector,\n",
        "    reducing dimensionality from ~42k to hash_dim (e.g., 256).\n",
        "    \"\"\"\n",
        "    retvals = retvals[:max_len]\n",
        "    freq = np.zeros(hash_dim)\n",
        "    for rv in retvals:\n",
        "        # Hash the return value to get bucket index\n",
        "        bucket = hash(str(rv)) % hash_dim\n",
        "        freq[bucket] += 1\n",
        "    total = len(retvals)\n",
        "    if total > 0:\n",
        "        freq = freq / total\n",
        "    return freq\n",
        "\n",
        "def extract_param_embedding(params, param_to_embedding, pad_embedding, max_len):\n",
        "    \"\"\"Extract aggregated parameter sentence embedding (mean pooling).\"\"\"\n",
        "    params = params[:max_len]\n",
        "    embeddings = []\n",
        "    for param_str in params:\n",
        "        if pd.isna(param_str):\n",
        "            key = '<EMPTY>'\n",
        "        else:\n",
        "            key = str(param_str)\n",
        "        embeddings.append(param_to_embedding.get(key, pad_embedding))\n",
        "    \n",
        "    if embeddings:\n",
        "        embeddings = np.array(embeddings)\n",
        "        return embeddings.mean(axis=0)\n",
        "    return pad_embedding\n",
        "\n",
        "def extract_combined_features(syscalls, retvals, params, \n",
        "                              syscall_encoder, retval_hash_dim,\n",
        "                              param_to_embedding, pad_embedding, max_len):\n",
        "    \"\"\"Extract and concatenate all three feature types.\n",
        "    \n",
        "    Returns:\n",
        "        Combined feature vector: [syscall_freq | retval_hashed | param_embed]\n",
        "    \"\"\"\n",
        "    syscall_feat = extract_syscall_frequency(syscalls, syscall_encoder, max_len)\n",
        "    retval_feat = extract_retval_hashed(retvals, retval_hash_dim, max_len)\n",
        "    param_feat = extract_param_embedding(params, param_to_embedding, pad_embedding, max_len)\n",
        "    \n",
        "    # Concatenate all features\n",
        "    return np.concatenate([syscall_feat, retval_feat, param_feat])\n",
        "\n",
        "print(f\"Feature extraction functions defined.\")\n",
        "print(f\"Combined feature dimensions: {syscall_vocab_size} (syscall) + {RETVAL_HASH_DIM} (retval hashed) + {EMBEDDING_DIM} (params)\")\n",
        "print(f\"Total feature dimension: {syscall_vocab_size + RETVAL_HASH_DIM + EMBEDDING_DIM}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "prepare_dataset",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using combined features: syscall frequency + return value hashed + parameter embeddings\n"
          ]
        }
      ],
      "source": [
        "def prepare_dataset_combined(file_label_pairs, syscall_encoder, retval_hash_dim,\n",
        "                              param_to_embedding, pad_embedding, max_len):\n",
        "    \"\"\"Prepare combined feature matrix and labels.\"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    label_map = {'benign': 0, 'malicious': 1}\n",
        "    \n",
        "    for path, label in file_label_pairs:\n",
        "        runs = load_runs_combined(path)\n",
        "        for syscalls, retvals, params in runs:\n",
        "            if len(syscalls) > 0:  # Need at least 1 syscall\n",
        "                features = extract_combined_features(\n",
        "                    syscalls, retvals, params,\n",
        "                    syscall_encoder, retval_hash_dim,\n",
        "                    param_to_embedding, pad_embedding, max_len\n",
        "                )\n",
        "                X.append(features)\n",
        "                y.append(label_map[label])\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "print(f\"Using combined features: syscall frequency + return value hashed + parameter embeddings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "experiments",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "EXPERIMENT: Window Size = 250\n",
            "============================================================\n",
            "Train samples: 1986, Test samples: 810\n",
            "Feature dimension: 720\n",
            "\n",
            "Training XGBoost...\n",
            "Training time: 0.71s\n",
            "\n",
            "Evaluating...\n",
            "Test time: 0.00s\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       0.99      0.82      0.90       489\n",
            "   malicious       0.78      0.98      0.87       321\n",
            "\n",
            "    accuracy                           0.89       810\n",
            "   macro avg       0.88      0.90      0.88       810\n",
            "weighted avg       0.91      0.89      0.89       810\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred: benign  Pred: malicious\n",
            "True: benign              402               87\n",
            "True: malicious             6              315\n",
            "\n",
            "Detection Rate: 0.9813\n",
            "False Positive Rate: 0.1779\n",
            "F1-score (weighted): 0.8864\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT: Window Size = 500\n",
            "============================================================\n",
            "Train samples: 1986, Test samples: 810\n",
            "Feature dimension: 720\n",
            "\n",
            "Training XGBoost...\n",
            "Training time: 0.58s\n",
            "\n",
            "Evaluating...\n",
            "Test time: 0.00s\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       1.00      0.81      0.90       489\n",
            "   malicious       0.78      1.00      0.87       321\n",
            "\n",
            "    accuracy                           0.89       810\n",
            "   macro avg       0.89      0.91      0.89       810\n",
            "weighted avg       0.91      0.89      0.89       810\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred: benign  Pred: malicious\n",
            "True: benign              398               91\n",
            "True: malicious             1              320\n",
            "\n",
            "Detection Rate: 0.9969\n",
            "False Positive Rate: 0.1861\n",
            "F1-score (weighted): 0.8876\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT: Window Size = 1000\n",
            "============================================================\n",
            "Train samples: 1986, Test samples: 810\n",
            "Feature dimension: 720\n",
            "\n",
            "Training XGBoost...\n",
            "Training time: 0.58s\n",
            "\n",
            "Evaluating...\n",
            "Test time: 0.00s\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       1.00      0.81      0.90       489\n",
            "   malicious       0.78      1.00      0.88       321\n",
            "\n",
            "    accuracy                           0.89       810\n",
            "   macro avg       0.89      0.91      0.89       810\n",
            "weighted avg       0.91      0.89      0.89       810\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred: benign  Pred: malicious\n",
            "True: benign              398               91\n",
            "True: malicious             0              321\n",
            "\n",
            "Detection Rate: 1.0000\n",
            "False Positive Rate: 0.1861\n",
            "F1-score (weighted): 0.8889\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT: Window Size = 2000\n",
            "============================================================\n",
            "Train samples: 1986, Test samples: 810\n",
            "Feature dimension: 720\n",
            "\n",
            "Training XGBoost...\n",
            "Training time: 0.60s\n",
            "\n",
            "Evaluating...\n",
            "Test time: 0.00s\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       0.82      0.81      0.82       489\n",
            "   malicious       0.72      0.74      0.73       321\n",
            "\n",
            "    accuracy                           0.78       810\n",
            "   macro avg       0.77      0.77      0.77       810\n",
            "weighted avg       0.78      0.78      0.78       810\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Pred: benign  Pred: malicious\n",
            "True: benign              398               91\n",
            "True: malicious            85              236\n",
            "\n",
            "Detection Rate: 0.7352\n",
            "False Positive Rate: 0.1861\n",
            "F1-score (weighted): 0.7831\n"
          ]
        }
      ],
      "source": [
        "# Run experiments with different window sizes\n",
        "results = []\n",
        "\n",
        "for window_size in WINDOW_SIZES:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXPERIMENT: Window Size = {window_size}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Prepare datasets\n",
        "    X_train, y_train = prepare_dataset_combined(\n",
        "        train_files, syscall_encoder, RETVAL_HASH_DIM,\n",
        "        param_to_embedding, PAD_EMBEDDING, window_size\n",
        "    )\n",
        "    X_test, y_test = prepare_dataset_combined(\n",
        "        test_files, syscall_encoder, RETVAL_HASH_DIM,\n",
        "        param_to_embedding, PAD_EMBEDDING, window_size\n",
        "    )\n",
        "    \n",
        "    print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
        "    print(f\"Feature dimension: {X_train.shape[1]}\")\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Create XGBoost model\n",
        "    model = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss'\n",
        "    )\n",
        "    \n",
        "    # Training\n",
        "    print(f\"\\nTraining XGBoost...\")\n",
        "    train_start_time = time.time()\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    train_time = time.time() - train_start_time\n",
        "    print(f\"Training time: {train_time:.2f}s\")\n",
        "    \n",
        "    # Testing\n",
        "    print(f\"\\nEvaluating...\")\n",
        "    test_start_time = time.time()\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    test_time = time.time() - test_start_time\n",
        "    print(f\"Test time: {test_time:.2f}s\")\n",
        "    \n",
        "    # Calculate metrics\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
        "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "    # Store results\n",
        "    results.append({\n",
        "        'Window Size': window_size,\n",
        "        'Detection Rate': detection_rate,\n",
        "        'False Positive Rate': false_positive_rate,\n",
        "        'F1-score (weighted)': f1_weighted,\n",
        "        'Train Time (s)': train_time,\n",
        "        'Test Time (s)': test_time\n",
        "    })\n",
        "    \n",
        "    # Print detailed results\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['benign', 'malicious']))\n",
        "    \n",
        "    labels = ['benign', 'malicious']\n",
        "    cm_df = pd.DataFrame(cm, index=[f'True: {l}' for l in labels], columns=[f'Pred: {l}' for l in labels])\n",
        "    print(f\"Confusion Matrix:\")\n",
        "    print(cm_df)\n",
        "    \n",
        "    print(f\"\\nDetection Rate: {detection_rate:.4f}\")\n",
        "    print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n",
        "    print(f\"F1-score (weighted): {f1_weighted:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "summary",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "SUMMARY OF RESULTS (Combined Features)\n",
            "================================================================================\n",
            " Window Size Detection Rate False Positive Rate F1-score (weighted) Train Time (s) Test Time (s)\n",
            "         250         0.9813              0.1779              0.8864           0.71          0.00\n",
            "         500         0.9969              0.1861              0.8876           0.58          0.00\n",
            "        1000         1.0000              0.1861              0.8889           0.58          0.00\n",
            "        2000         0.7352              0.1861              0.7831           0.60          0.00\n",
            "\n",
            "Feature composition:\n",
            "  - Syscall frequency histogram: 80 dimensions\n",
            "  - Return value (feature hashing): 256 dimensions\n",
            "  - Parameter sentence embeddings: 384 dimensions\n",
            "  - Total: 720 dimensions\n"
          ]
        }
      ],
      "source": [
        "# Summary Results Table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY OF RESULTS (Combined Features)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df['Detection Rate'] = results_df['Detection Rate'].apply(lambda x: f\"{x:.4f}\")\n",
        "results_df['False Positive Rate'] = results_df['False Positive Rate'].apply(lambda x: f\"{x:.4f}\")\n",
        "results_df['F1-score (weighted)'] = results_df['F1-score (weighted)'].apply(lambda x: f\"{x:.4f}\")\n",
        "results_df['Train Time (s)'] = results_df['Train Time (s)'].apply(lambda x: f\"{x:.2f}\")\n",
        "results_df['Test Time (s)'] = results_df['Test Time (s)'].apply(lambda x: f\"{x:.2f}\")\n",
        "\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(f\"\\nFeature composition:\")\n",
        "print(f\"  - Syscall frequency histogram: {syscall_vocab_size} dimensions\")\n",
        "print(f\"  - Return value (feature hashing): {RETVAL_HASH_DIM} dimensions\")\n",
        "print(f\"  - Parameter sentence embeddings: {EMBEDDING_DIM} dimensions\")\n",
        "print(f\"  - Total: {syscall_vocab_size + RETVAL_HASH_DIM + EMBEDDING_DIM} dimensions\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pt-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
