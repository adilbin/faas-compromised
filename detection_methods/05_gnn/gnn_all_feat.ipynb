{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fafc961",
   "metadata": {},
   "source": [
    "# Graph Convolutional Network with All Features Combined\n",
    "\n",
    "This notebook implements a Graph Convolutional Network (GCN) for syscall-based malware detection using **all features combined**:\n",
    "- **Syscall**: Categorical feature (embedded)\n",
    "- **Return Value (Ret)**: Categorical feature (embedded)\n",
    "- **Parameters**: Text feature (sentence transformer embeddings)\n",
    "\n",
    "## Graph Construction\n",
    "Each syscall sequence is modeled as a **temporal graph**:\n",
    "- **Nodes**: Each syscall in the sequence is a node\n",
    "- **Edges**: Consecutive syscalls are connected (temporal ordering)\n",
    "- **Node Features**: Concatenation of syscall, return value, and parameter embeddings\n",
    "\n",
    "The GCN learns to aggregate neighborhood information through message passing, then applies global pooling for graph-level classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91014e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adil-bb/anaconda3/envs/pt-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading sentence transformer model...\n",
      "Parameter embedding dimension: 384\n",
      "Total combined node feature dimension: 448\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../configs')\n",
    "from config_loader import get_split_with_labels\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch Geometric imports\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "# Config\n",
    "SPLIT = '70'\n",
    "WINDOW_SIZES = [250, 500, 1000, 2000]  # Different sliding window lengths to test\n",
    "# WINDOW_SIZES = [500]  # For quick testing\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Embedding dimensions\n",
    "SYSCALL_EMBED_DIM = 32\n",
    "RETVAL_EMBED_DIM = 32\n",
    "\n",
    "# Load sentence transformer model for parameter embeddings\n",
    "print(\"Loading sentence transformer model...\")\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "PARAM_EMBED_DIM = sentence_model.get_sentence_embedding_dimension()\n",
    "print(f\"Parameter embedding dimension: {PARAM_EMBED_DIM}\")\n",
    "print(f\"Total combined node feature dimension: {SYSCALL_EMBED_DIM + RETVAL_EMBED_DIM + PARAM_EMBED_DIM}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d55e727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 21, Test files: 9\n",
      "\n",
      "Training set:\n",
      "  Total runs: 1986\n",
      "  Benign runs: 1484\n",
      "  Malicious runs: 502\n",
      "\n",
      "Test set:\n",
      "  Total runs: 810\n",
      "  Benign runs: 489\n",
      "  Malicious runs: 321\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_files, test_files = get_split_with_labels(SPLIT)\n",
    "print(f\"Train files: {len(train_files)}, Test files: {len(test_files)}\")\n",
    "\n",
    "def load_runs_all_features(file_path):\n",
    "    \"\"\"Load all features (syscall, ret, parameters) grouped by run.\n",
    "    \n",
    "    Returns list of runs, where each run is a dict with:\n",
    "        - 'syscalls': list of syscall names\n",
    "        - 'retvals': list of return values\n",
    "        - 'params': list of parameter strings\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    runs = []\n",
    "    for run_id, group in df.groupby('run'):\n",
    "        run_data = {\n",
    "            'syscalls': group['syscall'].tolist(),\n",
    "            'retvals': group['Ret'].tolist(),\n",
    "            'params': group['parameters'].tolist()\n",
    "        }\n",
    "        runs.append(run_data)\n",
    "    return runs\n",
    "\n",
    "# Count runs per label for training and test sets\n",
    "def count_runs_per_label(file_label_pairs):\n",
    "    \"\"\"Count total runs per label.\"\"\"\n",
    "    counts = {'benign': 0, 'malicious': 0}\n",
    "    for path, label in file_label_pairs:\n",
    "        runs = load_runs_all_features(path)\n",
    "        counts[label] += len(runs)\n",
    "    return counts\n",
    "\n",
    "train_counts = count_runs_per_label(train_files)\n",
    "test_counts = count_runs_per_label(test_files)\n",
    "\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Total runs: {sum(train_counts.values())}\")\n",
    "print(f\"  Benign runs: {train_counts['benign']}\")\n",
    "print(f\"  Malicious runs: {train_counts['malicious']}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Total runs: {sum(test_counts.values())}\")\n",
    "print(f\"  Benign runs: {test_counts['benign']}\")\n",
    "print(f\"  Malicious runs: {test_counts['malicious']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "308c5ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoders for syscalls and return values...\n",
      "Syscall vocabulary size: 81 (including PAD)\n",
      "Return value vocabulary size: 42586 (including PAD)\n",
      "\n",
      "Unique parameter strings: 266019\n"
     ]
    }
   ],
   "source": [
    "# Build encoders for syscalls and return values\n",
    "print(\"Building encoders for syscalls and return values...\")\n",
    "\n",
    "all_syscalls = []\n",
    "all_retvals = []\n",
    "all_params = set()\n",
    "\n",
    "for path, _ in train_files + test_files:\n",
    "    for run_data in load_runs_all_features(path):\n",
    "        all_syscalls.extend(run_data['syscalls'])\n",
    "        all_retvals.extend(run_data['retvals'])\n",
    "        for param in run_data['params']:\n",
    "            if pd.isna(param):\n",
    "                all_params.add('<EMPTY>')\n",
    "            else:\n",
    "                all_params.add(str(param))\n",
    "\n",
    "# Build syscall encoder\n",
    "syscall_encoder = LabelEncoder()\n",
    "syscall_encoder.fit(all_syscalls)\n",
    "syscall_vocab_size = len(syscall_encoder.classes_) + 1  # +1 for PAD token\n",
    "print(f\"Syscall vocabulary size: {syscall_vocab_size} (including PAD)\")\n",
    "\n",
    "# Build return value encoder\n",
    "retval_encoder = LabelEncoder()\n",
    "retval_encoder.fit(all_retvals)\n",
    "retval_vocab_size = len(retval_encoder.classes_) + 1  # +1 for PAD token\n",
    "print(f\"Return value vocabulary size: {retval_vocab_size} (including PAD)\")\n",
    "\n",
    "# PAD index for embeddings\n",
    "PAD_IDX = 0\n",
    "\n",
    "print(f\"\\nUnique parameter strings: {len(all_params)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f0d9c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence embeddings for parameters (this may take a few minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1040/1040 [19:11<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter embeddings computed. Shape per embedding: 384\n"
     ]
    }
   ],
   "source": [
    "# Pre-compute sentence embeddings for all unique parameter strings\n",
    "print(\"Computing sentence embeddings for parameters (this may take a few minutes)...\")\n",
    "\n",
    "all_params_list = list(all_params)\n",
    "param_embeddings = sentence_model.encode(\n",
    "    all_params_list,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=256,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "# Create a mapping from parameter string to embedding\n",
    "param_to_embedding = {param: emb for param, emb in zip(all_params_list, param_embeddings)}\n",
    "print(f\"Parameter embeddings computed. Shape per embedding: {PARAM_EMBED_DIM}\")\n",
    "\n",
    "# Create zero embedding for padding\n",
    "PAD_PARAM_EMBEDDING = np.zeros(PARAM_EMBED_DIM, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c950da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing graph creation...\n",
      "Sample graph:\n",
      "  Num nodes: 100\n",
      "  Num edges: 198\n",
      "  Syscall indices shape: torch.Size([100])\n",
      "  Retval indices shape: torch.Size([100])\n",
      "  Param embeddings shape: torch.Size([100, 384])\n",
      "  Label: 0\n"
     ]
    }
   ],
   "source": [
    "def create_temporal_graph(syscalls, retvals, params, syscall_encoder, retval_encoder, \n",
    "                          param_to_embedding, window_size, label):\n",
    "    \"\"\"\n",
    "    Create a temporal graph from a syscall sequence.\n",
    "    \n",
    "    Graph structure:\n",
    "    - Each syscall in the sequence becomes a node\n",
    "    - Consecutive syscalls are connected by edges (temporal ordering)\n",
    "    - Node features: [syscall_idx, retval_idx, param_embedding]\n",
    "    \n",
    "    Returns:\n",
    "        PyG Data object with node features, edge index, and label\n",
    "    \"\"\"\n",
    "    # Truncate to window_size\n",
    "    seq_len = min(len(syscalls), window_size)\n",
    "    \n",
    "    syscalls = syscalls[:seq_len]\n",
    "    retvals = retvals[:seq_len]\n",
    "    params = params[:seq_len]\n",
    "    \n",
    "    # Encode syscalls and return values (+1 to reserve 0 for PAD)\n",
    "    encoded_syscalls = syscall_encoder.transform(syscalls) + 1\n",
    "    encoded_retvals = retval_encoder.transform(retvals) + 1\n",
    "    \n",
    "    # Get parameter embeddings\n",
    "    param_embs = []\n",
    "    for param in params:\n",
    "        if pd.isna(param):\n",
    "            key = '<EMPTY>'\n",
    "        else:\n",
    "            key = str(param)\n",
    "        param_embs.append(param_to_embedding[key])\n",
    "    param_embs = np.array(param_embs, dtype=np.float32)\n",
    "    \n",
    "    # Create edge index for temporal graph (consecutive connections)\n",
    "    # Edges: 0->1, 1->2, 2->3, ..., (n-2)->(n-1)\n",
    "    # For undirected graph, also add reverse edges\n",
    "    if seq_len > 1:\n",
    "        # Forward edges\n",
    "        src = list(range(seq_len - 1))\n",
    "        dst = list(range(1, seq_len))\n",
    "        # Backward edges (undirected)\n",
    "        src_back = list(range(1, seq_len))\n",
    "        dst_back = list(range(seq_len - 1))\n",
    "        # Combine\n",
    "        edge_index = torch.tensor([src + src_back, dst + dst_back], dtype=torch.long)\n",
    "    else:\n",
    "        # Single node graph - no edges\n",
    "        edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "    \n",
    "    # Create node features\n",
    "    # We'll store syscall and retval indices separately, then embed in the model\n",
    "    syscall_tensor = torch.tensor(encoded_syscalls, dtype=torch.long)\n",
    "    retval_tensor = torch.tensor(encoded_retvals, dtype=torch.long)\n",
    "    param_tensor = torch.tensor(param_embs, dtype=torch.float32)\n",
    "    \n",
    "    # Create PyG Data object\n",
    "    data = Data(\n",
    "        syscall_idx=syscall_tensor,\n",
    "        retval_idx=retval_tensor,\n",
    "        param_emb=param_tensor,\n",
    "        edge_index=edge_index,\n",
    "        y=torch.tensor([label], dtype=torch.long),\n",
    "        num_nodes=seq_len\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def create_graph_dataset(file_label_pairs, syscall_encoder, retval_encoder, \n",
    "                         param_to_embedding, window_size):\n",
    "    \"\"\"\n",
    "    Create a list of PyG Data objects from file_label_pairs.\n",
    "    \"\"\"\n",
    "    label_map = {'benign': 0, 'malicious': 1}\n",
    "    data_list = []\n",
    "    \n",
    "    for path, label in file_label_pairs:\n",
    "        runs = load_runs_all_features(path)\n",
    "        for run_data in runs:\n",
    "            graph = create_temporal_graph(\n",
    "                run_data['syscalls'],\n",
    "                run_data['retvals'],\n",
    "                run_data['params'],\n",
    "                syscall_encoder,\n",
    "                retval_encoder,\n",
    "                param_to_embedding,\n",
    "                window_size,\n",
    "                label_map[label]\n",
    "            )\n",
    "            data_list.append(graph)\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "\n",
    "# Test graph creation\n",
    "print(\"Testing graph creation...\")\n",
    "test_graphs = create_graph_dataset(train_files[:1], syscall_encoder, retval_encoder, \n",
    "                                   param_to_embedding, window_size=100)\n",
    "sample_graph = test_graphs[0]\n",
    "print(f\"Sample graph:\")\n",
    "print(f\"  Num nodes: {sample_graph.num_nodes}\")\n",
    "print(f\"  Num edges: {sample_graph.edge_index.shape[1]}\")\n",
    "print(f\"  Syscall indices shape: {sample_graph.syscall_idx.shape}\")\n",
    "print(f\"  Retval indices shape: {sample_graph.retval_idx.shape}\")\n",
    "print(f\"  Param embeddings shape: {sample_graph.param_emb.shape}\")\n",
    "print(f\"  Label: {sample_graph.y.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87e3d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNAllFeatures(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Network that combines all features:\n",
    "    - Syscall embeddings (learned)\n",
    "    - Return value embeddings (learned)\n",
    "    - Parameter embeddings (pre-computed sentence embeddings)\n",
    "    \n",
    "    Architecture:\n",
    "    1. Embed categorical features (syscall, retval)\n",
    "    2. Concatenate with parameter embeddings\n",
    "    3. Project to hidden dimension\n",
    "    4. Apply GCN layers for message passing\n",
    "    5. Global pooling (mean + max) for graph-level representation\n",
    "    6. Classification head\n",
    "    \"\"\"\n",
    "    def __init__(self, syscall_vocab_size, retval_vocab_size,\n",
    "                 syscall_embed_dim, retval_embed_dim, param_embed_dim,\n",
    "                 hidden_dim=128, num_gcn_layers=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.syscall_embed_dim = syscall_embed_dim\n",
    "        self.retval_embed_dim = retval_embed_dim\n",
    "        self.param_embed_dim = param_embed_dim\n",
    "        \n",
    "        # Embedding layers for categorical features\n",
    "        self.syscall_embedding = nn.Embedding(syscall_vocab_size, syscall_embed_dim, padding_idx=PAD_IDX)\n",
    "        self.retval_embedding = nn.Embedding(retval_vocab_size, retval_embed_dim, padding_idx=PAD_IDX)\n",
    "        \n",
    "        # Total input dimension after concatenation\n",
    "        total_embed_dim = syscall_embed_dim + retval_embed_dim + param_embed_dim\n",
    "        \n",
    "        # Project concatenated embeddings to hidden dimension\n",
    "        self.input_projection = nn.Linear(total_embed_dim, hidden_dim)\n",
    "        \n",
    "        # GCN layers\n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        self.gcn_layers.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        for _ in range(num_gcn_layers - 1):\n",
    "            self.gcn_layers.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # Batch normalization for each GCN layer\n",
    "        self.batch_norms = nn.ModuleList([\n",
    "            nn.BatchNorm1d(hidden_dim) for _ in range(num_gcn_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Classification head (using both mean and max pooling)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),  # *2 for mean + max pooling\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        syscall_idx = data.syscall_idx\n",
    "        retval_idx = data.retval_idx\n",
    "        param_emb = data.param_emb\n",
    "        edge_index = data.edge_index\n",
    "        batch = data.batch\n",
    "        \n",
    "        # Embed categorical features\n",
    "        syscall_emb = self.syscall_embedding(syscall_idx)  # (num_nodes, syscall_embed_dim)\n",
    "        retval_emb = self.retval_embedding(retval_idx)      # (num_nodes, retval_embed_dim)\n",
    "        \n",
    "        # Concatenate all embeddings\n",
    "        x = torch.cat([syscall_emb, retval_emb, param_emb], dim=1)  # (num_nodes, total_embed_dim)\n",
    "        \n",
    "        # Project to hidden dimension\n",
    "        x = self.input_projection(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply GCN layers with residual connections\n",
    "        for i, (gcn, bn) in enumerate(zip(self.gcn_layers, self.batch_norms)):\n",
    "            x_prev = x\n",
    "            x = gcn(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            # Residual connection\n",
    "            x = x + x_prev\n",
    "        \n",
    "        # Global pooling (combine mean and max for richer representation)\n",
    "        x_mean = global_mean_pool(x, batch)  # (batch_size, hidden_dim)\n",
    "        x_max = global_max_pool(x, batch)    # (batch_size, hidden_dim)\n",
    "        x = torch.cat([x_mean, x_max], dim=1)  # (batch_size, hidden_dim * 2)\n",
    "        \n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8460c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPERIMENT: Window Size = 250\n",
      "============================================================\n",
      "Creating graph datasets...\n",
      "Train samples: 1986, Test samples: 810\n",
      "\n",
      "--- Graph Statistics ---\n",
      "  Avg nodes per graph: 245.8\n",
      "  Avg edges per graph: 489.6\n",
      "  Max nodes (window size): 250\n",
      "\n",
      "--- Model Architecture & Parameters ---\n",
      "  syscall_embedding.weight: [81, 32] = 2,592 params\n",
      "  retval_embedding.weight: [42586, 32] = 1,362,752 params\n",
      "  input_projection.weight: [128, 448] = 57,344 params\n",
      "  input_projection.bias: [128] = 128 params\n",
      "  gcn_layers.0.bias: [128] = 128 params\n",
      "  gcn_layers.0.lin.weight: [128, 128] = 16,384 params\n",
      "  gcn_layers.1.bias: [128] = 128 params\n",
      "  gcn_layers.1.lin.weight: [128, 128] = 16,384 params\n",
      "  gcn_layers.2.bias: [128] = 128 params\n",
      "  gcn_layers.2.lin.weight: [128, 128] = 16,384 params\n",
      "  batch_norms.0.weight: [128] = 128 params\n",
      "  batch_norms.0.bias: [128] = 128 params\n",
      "  batch_norms.1.weight: [128] = 128 params\n",
      "  batch_norms.1.bias: [128] = 128 params\n",
      "  batch_norms.2.weight: [128] = 128 params\n",
      "  batch_norms.2.bias: [128] = 128 params\n",
      "  fc.0.weight: [128, 256] = 32,768 params\n",
      "  fc.0.bias: [128] = 128 params\n",
      "  fc.3.weight: [64, 128] = 8,192 params\n",
      "  fc.3.bias: [64] = 64 params\n",
      "  fc.6.weight: [2, 64] = 128 params\n",
      "  fc.6.bias: [2] = 2 params\n",
      "  ──────────────────────────────────────────────────\n",
      "  Total parameters:     1,514,402\n",
      "  Trainable parameters: 1,514,402\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 63/63 [00:06<00:00, 10.42it/s, loss=0.2730, acc=0.9003]\n",
      "Epoch 2/20: 100%|██████████| 63/63 [00:06<00:00, 10.43it/s, loss=0.0570, acc=0.9869]\n",
      "Epoch 3/20: 100%|██████████| 63/63 [00:05<00:00, 10.50it/s, loss=0.0261, acc=0.9945]\n",
      "Epoch 4/20: 100%|██████████| 63/63 [00:05<00:00, 10.55it/s, loss=0.0195, acc=0.9960]\n",
      "Epoch 5/20: 100%|██████████| 63/63 [00:06<00:00, 10.46it/s, loss=0.0170, acc=0.9955]\n",
      "Epoch 6/20: 100%|██████████| 63/63 [00:06<00:00, 10.44it/s, loss=0.0161, acc=0.9950]\n",
      "Epoch 7/20: 100%|██████████| 63/63 [00:06<00:00, 10.24it/s, loss=0.0166, acc=0.9935]\n",
      "Epoch 8/20: 100%|██████████| 63/63 [00:06<00:00, 10.47it/s, loss=0.0147, acc=0.9919]\n",
      "Epoch 9/20: 100%|██████████| 63/63 [00:06<00:00, 10.17it/s, loss=0.0162, acc=0.9950]\n",
      "Epoch 10/20: 100%|██████████| 63/63 [00:06<00:00, 10.43it/s, loss=0.0153, acc=0.9924]\n",
      "Epoch 11/20: 100%|██████████| 63/63 [00:06<00:00, 10.44it/s, loss=0.0078, acc=0.9950]\n",
      "Epoch 12/20: 100%|██████████| 63/63 [00:06<00:00, 10.42it/s, loss=0.0112, acc=0.9955]\n",
      "Epoch 13/20: 100%|██████████| 63/63 [00:06<00:00, 10.04it/s, loss=0.0100, acc=0.9960]\n",
      "Epoch 14/20: 100%|██████████| 63/63 [00:06<00:00,  9.93it/s, loss=0.0072, acc=0.9975]\n",
      "Epoch 15/20: 100%|██████████| 63/63 [00:06<00:00, 10.28it/s, loss=0.0059, acc=0.9980]\n",
      "Epoch 16/20: 100%|██████████| 63/63 [00:06<00:00, 10.13it/s, loss=0.0056, acc=0.9980]\n",
      "Epoch 17/20: 100%|██████████| 63/63 [00:06<00:00,  9.95it/s, loss=0.0074, acc=0.9965]\n",
      "Epoch 18/20: 100%|██████████| 63/63 [00:06<00:00, 10.07it/s, loss=0.0111, acc=0.9955]\n",
      "Epoch 19/20: 100%|██████████| 63/63 [00:06<00:00, 10.22it/s, loss=0.0045, acc=0.9985]\n",
      "Epoch 20/20: 100%|██████████| 63/63 [00:06<00:00, 10.24it/s, loss=0.0055, acc=0.9980]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 122.49s\n",
      "\n",
      "Evaluating...\n",
      "Test time: 0.79s\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.99      0.99      0.99       489\n",
      "   malicious       0.98      0.99      0.98       321\n",
      "\n",
      "    accuracy                           0.99       810\n",
      "   macro avg       0.99      0.99      0.99       810\n",
      "weighted avg       0.99      0.99      0.99       810\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred: benign  Pred: malicious\n",
      "True: benign              483                6\n",
      "True: malicious             4              317\n",
      "\n",
      "Detection Rate: 0.9875\n",
      "False Positive Rate: 0.0123\n",
      "F1-score (weighted): 0.9877\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: Window Size = 500\n",
      "============================================================\n",
      "Creating graph datasets...\n",
      "Train samples: 1986, Test samples: 810\n",
      "\n",
      "--- Graph Statistics ---\n",
      "  Avg nodes per graph: 377.6\n",
      "  Avg edges per graph: 753.1\n",
      "  Max nodes (window size): 500\n",
      "\n",
      "--- Model Architecture & Parameters ---\n",
      "  syscall_embedding.weight: [81, 32] = 2,592 params\n",
      "  retval_embedding.weight: [42586, 32] = 1,362,752 params\n",
      "  input_projection.weight: [128, 448] = 57,344 params\n",
      "  input_projection.bias: [128] = 128 params\n",
      "  gcn_layers.0.bias: [128] = 128 params\n",
      "  gcn_layers.0.lin.weight: [128, 128] = 16,384 params\n",
      "  gcn_layers.1.bias: [128] = 128 params\n",
      "  gcn_layers.1.lin.weight: [128, 128] = 16,384 params\n",
      "  gcn_layers.2.bias: [128] = 128 params\n",
      "  gcn_layers.2.lin.weight: [128, 128] = 16,384 params\n",
      "  batch_norms.0.weight: [128] = 128 params\n",
      "  batch_norms.0.bias: [128] = 128 params\n",
      "  batch_norms.1.weight: [128] = 128 params\n",
      "  batch_norms.1.bias: [128] = 128 params\n",
      "  batch_norms.2.weight: [128] = 128 params\n",
      "  batch_norms.2.bias: [128] = 128 params\n",
      "  fc.0.weight: [128, 256] = 32,768 params\n",
      "  fc.0.bias: [128] = 128 params\n",
      "  fc.3.weight: [64, 128] = 8,192 params\n",
      "  fc.3.bias: [64] = 64 params\n",
      "  fc.6.weight: [2, 64] = 128 params\n",
      "  fc.6.bias: [2] = 2 params\n",
      "  ──────────────────────────────────────────────────\n",
      "  Total parameters:     1,514,402\n",
      "  Trainable parameters: 1,514,402\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 63/63 [00:08<00:00,  7.10it/s, loss=0.3332, acc=0.8681]\n",
      "Epoch 2/20: 100%|██████████| 63/63 [00:08<00:00,  7.04it/s, loss=0.0222, acc=0.9965]\n",
      "Epoch 3/20: 100%|██████████| 63/63 [00:08<00:00,  7.07it/s, loss=0.0149, acc=0.9980]\n",
      "Epoch 4/20: 100%|██████████| 63/63 [00:09<00:00,  6.90it/s, loss=0.0092, acc=0.9985]\n",
      "Epoch 5/20: 100%|██████████| 63/63 [00:08<00:00,  7.05it/s, loss=0.0143, acc=0.9970]\n",
      "Epoch 6/20: 100%|██████████| 63/63 [00:09<00:00,  6.84it/s, loss=0.0073, acc=0.9985]\n",
      "Epoch 7/20: 100%|██████████| 63/63 [00:09<00:00,  6.90it/s, loss=0.0066, acc=0.9980]\n",
      "Epoch 8/20: 100%|██████████| 63/63 [00:09<00:00,  6.84it/s, loss=0.0093, acc=0.9990]\n",
      "Epoch 9/20: 100%|██████████| 63/63 [00:09<00:00,  6.95it/s, loss=0.0033, acc=0.9985]\n",
      "Epoch 10/20: 100%|██████████| 63/63 [00:09<00:00,  6.79it/s, loss=0.0028, acc=0.9995]\n",
      "Epoch 11/20: 100%|██████████| 63/63 [00:09<00:00,  6.87it/s, loss=0.0040, acc=0.9995]\n",
      "Epoch 12/20: 100%|██████████| 63/63 [00:09<00:00,  6.66it/s, loss=0.0029, acc=0.9990]\n",
      "Epoch 13/20: 100%|██████████| 63/63 [00:09<00:00,  6.82it/s, loss=0.0039, acc=0.9985]\n",
      "Epoch 14/20: 100%|██████████| 63/63 [00:09<00:00,  6.83it/s, loss=0.0016, acc=0.9995]\n",
      "Epoch 15/20: 100%|██████████| 63/63 [00:09<00:00,  6.86it/s, loss=0.0012, acc=0.9995]\n",
      "Epoch 16/20: 100%|██████████| 63/63 [00:09<00:00,  6.82it/s, loss=0.0017, acc=0.9995]\n",
      "Epoch 17/20: 100%|██████████| 63/63 [00:09<00:00,  6.79it/s, loss=0.0025, acc=0.9990]\n",
      "Epoch 18/20: 100%|██████████| 63/63 [00:09<00:00,  6.85it/s, loss=0.0039, acc=0.9990]\n",
      "Epoch 19/20: 100%|██████████| 63/63 [00:09<00:00,  6.84it/s, loss=0.0047, acc=0.9980]\n",
      "Epoch 20/20: 100%|██████████| 63/63 [00:09<00:00,  6.93it/s, loss=0.0031, acc=0.9985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 183.01s\n",
      "\n",
      "Evaluating...\n",
      "Test time: 1.34s\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       1.00      0.82      0.90       489\n",
      "   malicious       0.78      1.00      0.88       321\n",
      "\n",
      "    accuracy                           0.89       810\n",
      "   macro avg       0.89      0.91      0.89       810\n",
      "weighted avg       0.91      0.89      0.89       810\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred: benign  Pred: malicious\n",
      "True: benign              399               90\n",
      "True: malicious             1              320\n",
      "\n",
      "Detection Rate: 0.9969\n",
      "False Positive Rate: 0.1840\n",
      "F1-score (weighted): 0.8889\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: Window Size = 1000\n",
      "============================================================\n",
      "Creating graph datasets...\n",
      "Train samples: 1986, Test samples: 810\n",
      "\n",
      "--- Graph Statistics ---\n",
      "  Avg nodes per graph: 475.5\n",
      "  Avg edges per graph: 949.1\n",
      "  Max nodes (window size): 1000\n",
      "\n",
      "--- Model Architecture & Parameters ---\n",
      "  syscall_embedding.weight: [81, 32] = 2,592 params\n",
      "  retval_embedding.weight: [42586, 32] = 1,362,752 params\n",
      "  input_projection.weight: [128, 448] = 57,344 params\n",
      "  input_projection.bias: [128] = 128 params\n",
      "  gcn_layers.0.bias: [128] = 128 params\n",
      "  gcn_layers.0.lin.weight: [128, 128] = 16,384 params\n",
      "  gcn_layers.1.bias: [128] = 128 params\n",
      "  gcn_layers.1.lin.weight: [128, 128] = 16,384 params\n",
      "  gcn_layers.2.bias: [128] = 128 params\n",
      "  gcn_layers.2.lin.weight: [128, 128] = 16,384 params\n",
      "  batch_norms.0.weight: [128] = 128 params\n",
      "  batch_norms.0.bias: [128] = 128 params\n",
      "  batch_norms.1.weight: [128] = 128 params\n",
      "  batch_norms.1.bias: [128] = 128 params\n",
      "  batch_norms.2.weight: [128] = 128 params\n",
      "  batch_norms.2.bias: [128] = 128 params\n",
      "  fc.0.weight: [128, 256] = 32,768 params\n",
      "  fc.0.bias: [128] = 128 params\n",
      "  fc.3.weight: [64, 128] = 8,192 params\n",
      "  fc.3.bias: [64] = 64 params\n",
      "  fc.6.weight: [2, 64] = 128 params\n",
      "  fc.6.bias: [2] = 2 params\n",
      "  ──────────────────────────────────────────────────\n",
      "  Total parameters:     1,514,402\n",
      "  Trainable parameters: 1,514,402\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 63/63 [00:11<00:00,  5.41it/s, loss=0.3120, acc=0.8822]\n",
      "Epoch 2/20: 100%|██████████| 63/63 [00:11<00:00,  5.52it/s, loss=0.0835, acc=0.9713]\n",
      "Epoch 3/20: 100%|██████████| 63/63 [00:11<00:00,  5.34it/s, loss=0.0319, acc=0.9940]\n",
      "Epoch 4/20: 100%|██████████| 63/63 [00:11<00:00,  5.37it/s, loss=0.0097, acc=0.9985]\n",
      "Epoch 5/20: 100%|██████████| 63/63 [00:12<00:00,  5.18it/s, loss=0.0064, acc=0.9990]\n",
      "Epoch 6/20: 100%|██████████| 63/63 [00:11<00:00,  5.28it/s, loss=0.0043, acc=0.9985]\n",
      "Epoch 7/20: 100%|██████████| 63/63 [00:12<00:00,  5.12it/s, loss=0.0029, acc=0.9995]\n",
      "Epoch 8/20: 100%|██████████| 63/63 [00:11<00:00,  5.28it/s, loss=0.0027, acc=0.9995]\n",
      "Epoch 9/20: 100%|██████████| 63/63 [00:12<00:00,  5.16it/s, loss=0.0003, acc=1.0000]\n",
      "Epoch 10/20: 100%|██████████| 63/63 [00:12<00:00,  5.14it/s, loss=0.0009, acc=0.9995]\n",
      "Epoch 11/20: 100%|██████████| 63/63 [00:12<00:00,  5.15it/s, loss=0.0058, acc=0.9985]\n",
      "Epoch 12/20: 100%|██████████| 63/63 [00:11<00:00,  5.25it/s, loss=0.0105, acc=0.9985]\n",
      "Epoch 13/20: 100%|██████████| 63/63 [00:11<00:00,  5.26it/s, loss=0.0420, acc=0.9909]\n",
      "Epoch 14/20: 100%|██████████| 63/63 [00:12<00:00,  5.22it/s, loss=0.0022, acc=0.9990]\n",
      "Epoch 15/20: 100%|██████████| 63/63 [00:12<00:00,  5.24it/s, loss=0.0058, acc=0.9990]\n",
      "Epoch 16/20: 100%|██████████| 63/63 [00:12<00:00,  5.20it/s, loss=0.0006, acc=1.0000]\n",
      "Epoch 17/20: 100%|██████████| 63/63 [00:12<00:00,  5.24it/s, loss=0.0005, acc=1.0000]\n",
      "Epoch 18/20: 100%|██████████| 63/63 [00:12<00:00,  5.24it/s, loss=0.0004, acc=1.0000]\n",
      "Epoch 19/20: 100%|██████████| 63/63 [00:12<00:00,  5.19it/s, loss=0.0004, acc=1.0000]\n",
      "Epoch 20/20: 100%|██████████| 63/63 [00:11<00:00,  5.28it/s, loss=0.0098, acc=0.9955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 239.97s\n",
      "\n",
      "Evaluating...\n",
      "Test time: 1.86s\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       1.00      0.98      0.99       489\n",
      "   malicious       0.97      1.00      0.99       321\n",
      "\n",
      "    accuracy                           0.99       810\n",
      "   macro avg       0.99      0.99      0.99       810\n",
      "weighted avg       0.99      0.99      0.99       810\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred: benign  Pred: malicious\n",
      "True: benign              480                9\n",
      "True: malicious             0              321\n",
      "\n",
      "Detection Rate: 1.0000\n",
      "False Positive Rate: 0.0184\n",
      "F1-score (weighted): 0.9889\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: Window Size = 2000\n",
      "============================================================\n",
      "Creating graph datasets...\n",
      "Train samples: 1986, Test samples: 810\n",
      "\n",
      "--- Graph Statistics ---\n",
      "  Avg nodes per graph: 570.7\n",
      "  Avg edges per graph: 1139.5\n",
      "  Max nodes (window size): 2000\n",
      "\n",
      "--- Model Architecture & Parameters ---\n",
      "  syscall_embedding.weight: [81, 32] = 2,592 params\n",
      "  retval_embedding.weight: [42586, 32] = 1,362,752 params\n",
      "  input_projection.weight: [128, 448] = 57,344 params\n",
      "  input_projection.bias: [128] = 128 params\n",
      "  gcn_layers.0.bias: [128] = 128 params\n",
      "  gcn_layers.0.lin.weight: [128, 128] = 16,384 params\n",
      "  gcn_layers.1.bias: [128] = 128 params\n",
      "  gcn_layers.1.lin.weight: [128, 128] = 16,384 params\n",
      "  gcn_layers.2.bias: [128] = 128 params\n",
      "  gcn_layers.2.lin.weight: [128, 128] = 16,384 params\n",
      "  batch_norms.0.weight: [128] = 128 params\n",
      "  batch_norms.0.bias: [128] = 128 params\n",
      "  batch_norms.1.weight: [128] = 128 params\n",
      "  batch_norms.1.bias: [128] = 128 params\n",
      "  batch_norms.2.weight: [128] = 128 params\n",
      "  batch_norms.2.bias: [128] = 128 params\n",
      "  fc.0.weight: [128, 256] = 32,768 params\n",
      "  fc.0.bias: [128] = 128 params\n",
      "  fc.3.weight: [64, 128] = 8,192 params\n",
      "  fc.3.bias: [64] = 64 params\n",
      "  fc.6.weight: [2, 64] = 128 params\n",
      "  fc.6.bias: [2] = 2 params\n",
      "  ──────────────────────────────────────────────────\n",
      "  Total parameters:     1,514,402\n",
      "  Trainable parameters: 1,514,402\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 63/63 [00:14<00:00,  4.31it/s, loss=0.2584, acc=0.9104]\n",
      "Epoch 2/20: 100%|██████████| 63/63 [00:14<00:00,  4.26it/s, loss=0.0241, acc=0.9945]\n",
      "Epoch 3/20: 100%|██████████| 63/63 [00:14<00:00,  4.25it/s, loss=0.0184, acc=0.9965]\n",
      "Epoch 4/20: 100%|██████████| 63/63 [00:14<00:00,  4.21it/s, loss=0.0139, acc=0.9980]\n",
      "Epoch 5/20: 100%|██████████| 63/63 [00:14<00:00,  4.22it/s, loss=0.0052, acc=0.9985]\n",
      "Epoch 6/20: 100%|██████████| 63/63 [00:15<00:00,  4.15it/s, loss=0.0014, acc=0.9995]\n",
      "Epoch 7/20: 100%|██████████| 63/63 [00:15<00:00,  4.15it/s, loss=0.0030, acc=0.9995]\n",
      "Epoch 8/20: 100%|██████████| 63/63 [00:15<00:00,  4.14it/s, loss=0.0033, acc=0.9995]\n",
      "Epoch 9/20: 100%|██████████| 63/63 [00:15<00:00,  4.13it/s, loss=0.0183, acc=0.9940]\n",
      "Epoch 10/20: 100%|██████████| 63/63 [00:15<00:00,  4.15it/s, loss=0.0057, acc=0.9985]\n",
      "Epoch 11/20: 100%|██████████| 63/63 [00:15<00:00,  4.09it/s, loss=0.0061, acc=0.9990]\n",
      "Epoch 12/20: 100%|██████████| 63/63 [00:15<00:00,  4.13it/s, loss=0.0036, acc=0.9995]\n",
      "Epoch 13/20: 100%|██████████| 63/63 [00:15<00:00,  4.10it/s, loss=0.0006, acc=1.0000]\n",
      "Epoch 14/20: 100%|██████████| 63/63 [00:15<00:00,  4.08it/s, loss=0.0009, acc=0.9995]\n",
      "Epoch 15/20: 100%|██████████| 63/63 [00:15<00:00,  4.05it/s, loss=0.0003, acc=1.0000]\n",
      "Epoch 16/20: 100%|██████████| 63/63 [00:15<00:00,  4.06it/s, loss=0.0001, acc=1.0000]\n",
      "Epoch 17/20: 100%|██████████| 63/63 [00:15<00:00,  4.05it/s, loss=0.0003, acc=1.0000]\n",
      "Epoch 18/20: 100%|██████████| 63/63 [00:15<00:00,  4.14it/s, loss=0.0001, acc=1.0000]\n",
      "Epoch 19/20: 100%|██████████| 63/63 [00:15<00:00,  4.07it/s, loss=0.0001, acc=1.0000]\n",
      "Epoch 20/20: 100%|██████████| 63/63 [00:15<00:00,  4.05it/s, loss=0.0002, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 304.64s\n",
      "\n",
      "Evaluating...\n",
      "Test time: 2.35s\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       1.00      1.00      1.00       489\n",
      "   malicious       0.99      1.00      1.00       321\n",
      "\n",
      "    accuracy                           1.00       810\n",
      "   macro avg       1.00      1.00      1.00       810\n",
      "weighted avg       1.00      1.00      1.00       810\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred: benign  Pred: malicious\n",
      "True: benign              487                2\n",
      "True: malicious             0              321\n",
      "\n",
      "Detection Rate: 1.0000\n",
      "False Positive Rate: 0.0041\n",
      "F1-score (weighted): 0.9975\n"
     ]
    }
   ],
   "source": [
    "# Run experiments with different window sizes\n",
    "results = []\n",
    "\n",
    "for window_size in WINDOW_SIZES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXPERIMENT: Window Size = {window_size}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create graph datasets\n",
    "    print(\"Creating graph datasets...\")\n",
    "    train_graphs = create_graph_dataset(\n",
    "        train_files, syscall_encoder, retval_encoder,\n",
    "        param_to_embedding, window_size\n",
    "    )\n",
    "    test_graphs = create_graph_dataset(\n",
    "        test_files, syscall_encoder, retval_encoder,\n",
    "        param_to_embedding, window_size\n",
    "    )\n",
    "    \n",
    "    # Create PyG DataLoaders\n",
    "    train_loader = PyGDataLoader(train_graphs, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = PyGDataLoader(test_graphs, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    print(f\"Train samples: {len(train_graphs)}, Test samples: {len(test_graphs)}\")\n",
    "    \n",
    "    # Print graph statistics\n",
    "    avg_nodes = np.mean([g.num_nodes for g in train_graphs])\n",
    "    avg_edges = np.mean([g.edge_index.shape[1] for g in train_graphs])\n",
    "    print(f\"\\n--- Graph Statistics ---\")\n",
    "    print(f\"  Avg nodes per graph: {avg_nodes:.1f}\")\n",
    "    print(f\"  Avg edges per graph: {avg_edges:.1f}\")\n",
    "    print(f\"  Max nodes (window size): {window_size}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = GCNAllFeatures(\n",
    "        syscall_vocab_size=syscall_vocab_size,\n",
    "        retval_vocab_size=retval_vocab_size,\n",
    "        syscall_embed_dim=SYSCALL_EMBED_DIM,\n",
    "        retval_embed_dim=RETVAL_EMBED_DIM,\n",
    "        param_embed_dim=PARAM_EMBED_DIM,\n",
    "        hidden_dim=128,\n",
    "        num_gcn_layers=3,\n",
    "        dropout=0.3\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Print model parameter sizes per layer\n",
    "    print(f\"\\n--- Model Architecture & Parameters ---\")\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        param_count = param.numel()\n",
    "        total_params += param_count\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param_count\n",
    "        print(f\"  {name}: {list(param.shape)} = {param_count:,} params\")\n",
    "    print(f\"  {'─'*50}\")\n",
    "    print(f\"  Total parameters:     {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Training\n",
    "    print(f\"\\nTraining...\")\n",
    "    train_start_time = time.time()\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=True)\n",
    "        for data in pbar:\n",
    "            data = data.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += (predicted == data.y).sum().item()\n",
    "            total += data.num_graphs\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{total_loss/total:.4f}', 'acc': f'{correct/total:.4f}'})\n",
    "        \n",
    "        train_acc = correct / total\n",
    "        avg_loss = total_loss / total\n",
    "    \n",
    "    train_time = time.time() - train_start_time\n",
    "    print(f\"Training time: {train_time:.2f}s\")\n",
    "    \n",
    "    # Testing\n",
    "    print(f\"\\nEvaluating...\")\n",
    "    test_start_time = time.time()\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(DEVICE)\n",
    "            outputs = model(data)\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(data.y.cpu().numpy())\n",
    "    \n",
    "    test_time = time.time() - test_start_time\n",
    "    print(f\"Test time: {test_time:.2f}s\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Window Size': window_size,\n",
    "        'Detection Rate': detection_rate,\n",
    "        'False Positive Rate': false_positive_rate,\n",
    "        'F1-score (weighted)': f1_weighted,\n",
    "        'Train Time (s)': train_time,\n",
    "        'Test Time (s)': test_time\n",
    "    })\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['benign', 'malicious']))\n",
    "    \n",
    "    labels_names = ['benign', 'malicious']\n",
    "    cm_df = pd.DataFrame(cm, index=[f'True: {l}' for l in labels_names], columns=[f'Pred: {l}' for l in labels_names])\n",
    "    print(f\"Confusion Matrix:\")\n",
    "    print(cm_df)\n",
    "    \n",
    "    print(f\"\\nDetection Rate: {detection_rate:.4f}\")\n",
    "    print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n",
    "    print(f\"F1-score (weighted): {f1_weighted:.4f}\")\n",
    "    \n",
    "    # Clear GPU memory between experiments\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54301e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY OF RESULTS (GCN - All Features Combined)\n",
      "================================================================================\n",
      " Window Size Detection Rate False Positive Rate F1-score (weighted) Train Time (s) Test Time (s)\n",
      "         250         0.9875              0.0123              0.9877         122.49          0.79\n",
      "         500         0.9969              0.1840              0.8889         183.01          1.34\n",
      "        1000         1.0000              0.0184              0.9889         239.97          1.86\n",
      "        2000         1.0000              0.0041              0.9975         304.64          2.35\n",
      "\n",
      "Graph Construction:\n",
      "  - Node type: Syscall event\n",
      "  - Edge type: Temporal (consecutive syscalls)\n",
      "  - Graph type: Undirected\n",
      "\n",
      "Node Features combined:\n",
      "  - Syscall embedding: 32 dims\n",
      "  - Return value embedding: 32 dims\n",
      "  - Parameter embedding (sentence transformer): 384 dims\n",
      "  - Total node features: 448 dims\n",
      "  - Projected to hidden dim: 128 dims\n",
      "\n",
      "GCN Architecture:\n",
      "  - Number of GCN layers: 3\n",
      "  - Hidden dimension: 128\n",
      "  - Pooling: Mean + Max\n",
      "  - Dropout: 0.3\n",
      "  - Residual connections: Yes\n"
     ]
    }
   ],
   "source": [
    "# Summary Results Table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY OF RESULTS (GCN - All Features Combined)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['Detection Rate'] = results_df['Detection Rate'].apply(lambda x: f\"{x:.4f}\")\n",
    "results_df['False Positive Rate'] = results_df['False Positive Rate'].apply(lambda x: f\"{x:.4f}\")\n",
    "results_df['F1-score (weighted)'] = results_df['F1-score (weighted)'].apply(lambda x: f\"{x:.4f}\")\n",
    "results_df['Train Time (s)'] = results_df['Train Time (s)'].apply(lambda x: f\"{x:.2f}\")\n",
    "results_df['Test Time (s)'] = results_df['Test Time (s)'].apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nGraph Construction:\")\n",
    "print(f\"  - Node type: Syscall event\")\n",
    "print(f\"  - Edge type: Temporal (consecutive syscalls)\")\n",
    "print(f\"  - Graph type: Undirected\")\n",
    "\n",
    "print(f\"\\nNode Features combined:\")\n",
    "print(f\"  - Syscall embedding: {SYSCALL_EMBED_DIM} dims\")\n",
    "print(f\"  - Return value embedding: {RETVAL_EMBED_DIM} dims\")\n",
    "print(f\"  - Parameter embedding (sentence transformer): {PARAM_EMBED_DIM} dims\")\n",
    "print(f\"  - Total node features: {SYSCALL_EMBED_DIM + RETVAL_EMBED_DIM + PARAM_EMBED_DIM} dims\")\n",
    "print(f\"  - Projected to hidden dim: 128 dims\")\n",
    "\n",
    "print(f\"\\nGCN Architecture:\")\n",
    "print(f\"  - Number of GCN layers: 3\")\n",
    "print(f\"  - Hidden dimension: 128\")\n",
    "print(f\"  - Pooling: Mean + Max\")\n",
    "print(f\"  - Dropout: 0.3\")\n",
    "print(f\"  - Residual connections: Yes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
